<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Data-Driven RetinaNet Model for Small Object Detection in Aerial Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-09-03">3 Sep 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhicheng</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Missouri Columbia</orgName>
								<address>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinwen</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Missouri Columbia</orgName>
								<address>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yi</forename><surname>Shang</surname></persName>
							<email>shangy@umsystem.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Missouri Columbia</orgName>
								<address>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Data-Driven RetinaNet Model for Small Object Detection in Aerial Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-09-03">3 Sep 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">8184269C7957977570449805898ECEA7</idno>
					<idno type="arXiv">arXiv:2509.02928v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-09-05T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computer vision</term>
					<term>aerial image analysis</term>
					<term>small object detection</term>
					<term>deep learning</term>
					<term>transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the realm of aerial imaging, the ability to detect small objects is pivotal for a myriad of applications, encompassing environmental surveillance, urban design, and crisis management. Leveraging RetinaNet, this work unveils DDR-Net: a datadriven, deep-learning model devised to enhance the detection of diminutive objects. DDR-Net introduces novel, data-driven techniques to autonomously ascertain optimal feature maps and anchor estimations, cultivating a tailored and proficient training process while maintaining precision. Additionally, this paper presents an innovative sampling technique to bolster model efficacy under limited data training constraints. The model's enhanced detection capabilities support critical applications including wildlife and habitat monitoring, traffic flow optimization, and public safety improvements through accurate identification of small objects like vehicles and pedestrians. DDR-Net significantly reduces the cost and time required for data collection and training, offering efficient performance even with limited data. Empirical assessments over assorted aerial avian imagery datasets demonstrate that DDR-Net markedly surpasses RetinaNet and alternative contemporary models. These innovations advance current aerial image analysis technologies and promise wideranging impacts across multiple sectors including agriculture, security, and archaeology.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Aerial image analysis plays a important role in many applications such as environmental monitoring, urban planning, agricultural management, and disaster response and recovery. High-resolution aerial imagery captured by drones provides wide-ranging views, supplying rich datasets for monitoring, analysis, and informed decision-making. Yet, the large amount of data generated from aerial surveys present a significant challenge for manual analysis. Therefore, the application of Machine Learning (ML) techniques, especially state-of-theart deep learning models, is essential to automate the data processing pipeline, extract valuable insights, and speed up decision-making processes.</p><p>A key to the success of applications of ML, especially deep learning models, in aerial image analysis is having ample annotated datasets for training robust models. However, collecting such datasets is often prohibitively time-consuming and expensive. The diversity and complexity of aerial imagery further increase the data requirement for training models capable of generalizing well to unseen data, thus slowing the progress towards fully automated aerial image analysis systems.</p><p>Several strategies have been suggested in literature to address the data scarcity issue. Among these, transfer learning and data augmentation stand out, as they utilize existing knowledge and synthetic data augmentation to enhance model training with limited data <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. Despite these efforts, there's still a considerable room for improvement, especially in developing architectures that are adept at learning from limited data.</p><p>In this paper, we present a new architecture based on RetinaNet <ref type="bibr" target="#b3">[4]</ref>, named DDR-Net, to improve how our model performs when trained with a small amount of data. Unlike existing models like RetinaNet, DDR-Net takes advantages of the specific features of small objects such as birds in aerial images and can reach a good performance with a limited amount of training examples. The main contributions of the paper are as follows:</p><p>1) Propose a new method to determine appropriate sizes of feature maps and reduce the number of negative samples needed to speed up model training. 2) Propose a new method to automatically choose the number and size of anchor boxes, which are aligned well with real bounding boxes. 3) Propose a new clustering method for data sampling, aiming to get balanced representative training data. Our extensive experiments on multiple public datasets show that DDR-Net performed better than RetinaNet and some existing methods.</p><p>The rest of this paper is organized as follows: Section II presents related work of object detection in aerial images. Section III describes the proposed DDR-Net architecture and proposed new methods. Section IV presents the experimental setup and results. Finally, Section V concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Object detection is a common task in analyzing aerial images, supporting various real-world applications, such as investigating forest conservation areas <ref type="bibr" target="#b5">[5]</ref>- <ref type="bibr" target="#b7">[7]</ref> or monitoring waterfowl <ref type="bibr" target="#b8">[8]</ref>- <ref type="bibr" target="#b10">[10]</ref>. A lot of these applications use one-stage object detectors, i.e. deep learning models for end-to-end object detection such as RetinaNet, due to their fast speeds. Moreover, their simple configurable structures make the models easy to setup. Onestage detectors have been adaptable to different use cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a) Optimizing Performance with Limited Training Data:</head><p>The acquisition of substantial training data, particularly highquality annotations, is a pivotal step in developing robust object detection models. However, when constrained by the amount of available data, employing data augmentation techniques becomes widely used to enhance a deep learning model's learning capability and, consequently, its performance. The work by <ref type="bibr" target="#b11">[11]</ref> provides a comprehensive review of existing data augmentation techniques tailored for object detection tasks. Data augmentation techniques, such as geometric transformations and color space augmentations, artificially expand the existing dataset, thereby enriching a model's learning environment and improving its performance even with limited data. Data augmentation is particularly beneficial in real-world scenarios such as medical imaging, where data is often scarce. b) Adapting to Varying Target Scales: The tasks of object detection in aerial imagery often encounter a broad range of object sizes. Several methods have been proposed to enhance model performance across diverse object scales. One such method is adaptive anchor calculation <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, which is commonly employed in anchor-based methods. In this technique, the system iterates through the training dataset to determine the optimal anchor settings that align with the data. This method notably boosts the speed and accuracy of object detection by choosing appropriate anchor boxes through a process like the K-means clustering algorithm, thereby creating sets of adaptive anchor boxes that better match the object size distribution in the dataset <ref type="bibr" target="#b14">[14]</ref>.</p><p>Moreover, Adaptive Anchor Box Optimization (AABO) has been introduced as a way to redefine the shapes and sizes of anchor boxes, veering away from pre-defined configurations, and allowing more flexibility in dealing with varied object scales. This is especially pertinent since pre-defined anchor configurations may not always match well with the object size distributions in specific datasets <ref type="bibr" target="#b11">[11]</ref>. c) Refining Proposals During Training: Another approach to improve model performance is reducing the number of proposals used during loss calculation during model training. In the original setup of RetinaNet, a matching threshold is set based on IOU (Intersection over Union) between real bounding box and anchor boxes to lessen the number of anchors involved in the final calculation. This threshold might change with different training data. <ref type="bibr" target="#b15">[15]</ref> proposed a Multi-Level Feature Fusion Module (MFFM) and a Multi-Scale Feature Fusion Unit (MFFU) to tackle the problem of not using features well in pedestrian detection, an idea that can be applied to detecting objects from aerial images. <ref type="bibr" target="#b16">[16]</ref> proposed a parallel feature fusion technique using complex vectors for a joined-up feature representation, significantly improving classification performance when compared to the usual serial feature fusion methods. This way of representing data could be very helpful in dealing with different object sizes and complex backgrounds in aerial imagery. Moreover, how data is represented when sampling training data is key for effective learning, highlighting the importance of smart data sampling strategies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHODS</head><p>Traditional deep learning models for object detection employ preset anchors with predefined sizes and aspect ratios, aiming to adapt to a wide range of scale variance. However, in aerial images of certain targets, such birds, target sizes exhibit lesser variance, tending to fall within a specific range determined by the Ground Sampling Distance (GSD) of the image, also known as the scale of the image. Furthermore, substantial target information can be used when detecting specific object types, although the scale may vary significantly due to differences in data collection processes such as flying altitude and camera specifications of drones. We propose DDR-Net, a new architecture based on RetinaNet, to take advantage of the useful information.</p><p>The architecture of DDR-Net is designed to utilize training data in guiding architectural decisions, aiming to reduce computational overhead while enhancing model performance in both training from scratch and fine-tuning scenarios. DDR-Net has three new parts: Adaptive Feature Map Construction, Anchor Box Estimation, and ML-based Image Sampling. The architecture of DDR-Net, as compared to that of RetinaNet, is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>DDR-Net's design embraces the inherent data structures and features of aerial imagery and requires fewer training samples to achieve more accurate and efficient object detection, particularly when dealing with variations in image scale arising from differing data collection parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Adaptive Feature Map Construction</head><p>Conventional models like the Feature Pyramid Network (FPN) generate predictions from various feature layers. When running a pre-trained model of RetinaNet on certain targets, most predictions for objects might come from the anchors in one layer, such as the P4 layer, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. However, there are problems when fine-tuning this model on similar objects at different scales. For example, smaller objects that fit better with the P3 layer may not gain much from pre-trained information geared towards P4. This disconnect stems from the scale shift and different anchor alignments between layers. Unlike traditional FPN, DDR-Net introduces an adaptive feature map estimation module to tackle scale variations. Integrated with a fusion layer, this module dynamically merges multiple feature maps, scaling them to an optimal size. This way, useful features from earlier training can be effectively transferred to a new dataset, despite scale differences. The fusion layer, a unique aspect of DDR-Net, is designed to maximize the usefulness of pre-trained model features during fine-tuning, even with differing target sizes. DDR-Net also addresses overfitting or underfitting issues, which are common in aerial imagery where object sizes are more uniform due to the fixed camera position. It dynamically picks the best feature map dimensions based on the training dataset, aiming to choose sizes that provide enough anchor boxes for predictions while cutting down on negative samples.</p><p>The procedure for finding the optimal feature map size is shown in Algorithm 1. The Matching Score Calculation method is shown in Algorithm 2. Using the Everglade dataset <ref type="bibr" target="#b17">[17]</ref> as an example, we calculated the feature map score with a weighting factor of -1 and penalty factor of 0.0001. The resulting scores for pyramid levels P3 to P7 of RetinaNet were: 40020.7, 44785.7, 44018.8, 38753.3, and 21990.1, respectively. Among these, P4 stood out as the best, determining the fused feature map size. Fig. <ref type="figure" target="#fig_1">2</ref> shows an example that illustrates feature map size from P3 to P6 of RetinaNet. Ideally, we want each anchor to match no more than a few bounding boxes (defined by the anchor estimation process) without including too many negative anchors. By observing how each annotation fits within the grid, it is evident that the P4 layer generates the best result. This adaptive feature map technique enhances DDR-Net's adaptability and versatility across a variety of aerial image scenarios, making it useful for accurate object detection and smooth fine-tuning across diverse datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Anchor Box Estimation</head><p>Traditional object detection models typically employ preset anchors with predefined sizes and aspect ratios to adapt to a wide range of scale variances. However, in aerial images of certain applications, such as bird detection, the variance in target sizes is often lesser due to similar ground sampling distances (GSD) of the images, also referred to as the scale of the images. For such scenarios, we propose the estimation of anchor box sizes based on training data, replacing the preset anchor boxes.</p><p>We use K-means clustering algorithm to group the bounding boxes from the training dataset into clusters. The silhouette score is employed to guide the selection of optimal number of clusters. Initially, we collect all ground-truth bounding boxes and stores their widths and heights. Next, we record the silhouette score for number of clusters ranging from 2 to 6, representing the number of clusters being tested. Finally, we select the cluster number that maximizes the silhouette score. Fig. <ref type="figure" target="#fig_2">3</ref> shows an example where three clusters are the best. The proposed method for estimating anchor box sizes has two benefits. One is the reduction in effort required for calculating the bounding box regression loss. This is achieved by customizing the anchor boxes based on the distribution of ground-truth bounding boxes which in turn minimizes the regression loss between the anchor boxes and the actual bounding boxes. The second benefit is the facilitation of bridging the scale variance between different targets during transfer learning. This is further explained in the adaptive feature map section where features from different layers are merged into a single feature map.</p><p>Let's use a simple example to illustrate the benefits by using two different bird datasets: one from the Everglades dataset with a bounding box size of 50 × 50, and the other from the seabirdwatch dataset with a size of 20 × 20 <ref type="bibr" target="#b17">[17]</ref>. </p><p>where w and h are the width and height of the real bounding box, while w anchor and h anchor are the width and height of the predicted anchor box. • In our method of estimating anchor boxes, a clustering algorithm gives us anchor boxes sized 50×50 and 20×20 respectively. Using the same equations as before, the loss on the anchor width and height becomes zero, a perfect match.</p><p>This example shows how our way of estimating anchor boxes can lower the regression loss and handle different target sizes well during transfer learning.</p><p>Also, in a transfer learning scenario, if we want to finetune models pre-trained using the Everglades dataset to the seabirdwatch dataset, using our estimated anchors makes the regression loss difference smaller compared to the traditional setup: from 0 to 0, as compared to from -0.358 to 0.223.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ML-based Image Sampling for Training</head><p>In machine learning, it is crucial to have a balanced mix of targets in training data to train good models. For example, in the example from the seabirdwatch dataset shown in Fig. <ref type="figure" target="#fig_3">4</ref>, there are both black and white birds. When we trained a model using 1000 randomly selected annotations, for different mix of black and white birds in the training set, the model had quite different test performances.</p><p>Inspired by the clustering methods used in data sampling <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, we propose a method to select a small subset of the training data that can gives us a good balanced representation of the targets in the training set. Specifically, the method is as follows.</p><p>1) Use a base model, such as ResNet-50 pre-trained on some dataset like COCO, to create features for each image. The features could be the values of the layer before the output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Use a dimension reduction method, such as Principal</head><p>Component Analysis (PCA) to reduce the features to smaller size. 3) Generate additional features, such as the estimated number of bounding boxes from an existing detector, the type of image background from an existing classifier, and other useful information from the image metadata. 4) Run a clustering method, such as K-means clustering, to group training images into clusters based on their features. Use silhouette score to find the optimal number of clusters. 5) Given a target number of images per cluster, randomly select the target number of images from each cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we present experimental results of the proposed DDR-Net, and compare its performance with previous methods. Two performance metrics are used: best F1 score and mean average precision (mAP). The best F1 score is the highest F1 score over different confidence thresholds. Mean average precision (mAP) is a standard performance metric for object detection tasks, offering a comprehensive view of model performance across all confidence thresholds and quantifying the model's ability to correctly identify and localize objects under varying degrees of overlap and occlusion. In our experiments, for both F1 score and mAP calculations, an Intersection over Union (IoU) threshold of 0.3 was used and measured across all the experiments.</p><p>In the experiments, we used public datasets used in the bird detector study <ref type="bibr" target="#b17">[17]</ref> for performance comparison. The datasets are listed in Table <ref type="table" target="#tab_1">I</ref> here as we mentioned in previous section, GSD stands for Ground Sampling Distance and representing the image scale difference across different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance Comparison with Bird Detector</head><p>In this study, we conducted a comparative analysis between our DDR-Net model and the "Bird Detector" model described in reference <ref type="bibr" target="#b17">[17]</ref>, a model that the original authors have openly shared. While reference <ref type="bibr" target="#b17">[17]</ref> outlines an evaluation strategy where the Bird Detector model is pre-trained on datas from all available training sets except for the target datasetI, and subsequently fine-tuned with the training set from the target dataset, our experimental design adopts a different approach.</p><p>For a more direct and isolated comparison, we implemented a Local-only setup, where both DDR-Net and Bird Detector were individually trained solely on the training set of each target dataset. This approach eliminates the influence of external data and allows for a clear-cut performance assessment of each model based exclusively on the target dataset's test set.</p><p>For our DDR-Net training, we started FPN part of the model with pre-trained FPN weights from COCO dataset and random weights for the rest of the model. We trained the model for 80 epochs with learning rate 0.001 and batch size 24. We used basic image augmentations, e.g, flipping, scaling, rotating, etc. The same settings were used in all experiments. The experiments were done on a Dell desktop computer with an RTX 4090 graphics card.</p><p>Table <ref type="table" target="#tab_1">II</ref> shows performance comparison of the two models on the datasets. Out of the nine datasets, DDR-Net is significantly better on six datasets, about the same on two datasets, and slightly worse on one. DDR-Net is from 12% to 27.8% faster in inference. This inference speed gain mainly resulted from fewer proposals generated by our adaptive feature map calculation as compared with standard feature map proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance of ML-based Image Sampling Method</head><p>In this experiment, we evaluate the ML-based Image Sampling method by comparing the performances of DDR-Net models trained using training sets generated by this method with those of DDR-Net models trained using unbalanced (or biased) training sets. We conducted 10 random trials, and in each trial, we restricted the training data to 1000 annotations.</p><p>To </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance of DDR-Net Fine-Tuned with Limited Data</head><p>In this section, we compare the performances of DDR-Net and Bird-Detector when fine-tuned with a limited dataset. We limited the training set to a total of 1,000 annotations. First, we trained a DDR-Net model and a Bird-Detector model using the Everglades dataset. These two models were fine-tuned on the other nine datasets.</p><p>When fine-tuning DDR-Net models, we fixed feature map sizes and number of anchor boxes to avoid any weight mismatch between the pre-trained and fine-tuned model weights. During fine-tuning, we set the training epochs to 20 with learning rate 0.002, the same as <ref type="bibr" target="#b17">[17]</ref>. We ensured a fair comparison by keeping the sampled training data identical in each trial for both models. This experiment was repeated 10 times to get the average performance and variations. After fine-tuning, the models were evaluated on the test set of each dataset. Fig. <ref type="figure" target="#fig_8">6</ref> and Fig. <ref type="figure">7</ref> compare the performance of fine-tuned Bird Detector and DDR-Net models on nine datasets. The average and standard deviation of the best F1 score and mAP for different datasets are shown. Overall, DDR-Net is significantly better than Bird Detector on 4 datasets and is similar on 5 datasets. For the penguins dataset, the Bird Detector model failed to be fine-tuned. In terms of mAP values, DDR-Net is better on all datasets. For example, on the monash dataset, DDR-Net achieved 63.65 while Bird Detector was 45.23.</p><p>Table <ref type="table" target="#tab_2">III</ref> shows the average gains of DDR-Net models and P-values of paired-t test results between the mAP values of finetuned Bird Detector and DDR-Net model across the datasets, where the training data were identical between the two models. The improvement of DDR-Net is statistically significant.  showed significant improvement over the state-of-the-art Bird Detector model. Fine-tuning experiments showed that DDR-Net outperformed Bird Detector using limited amount of training data. The ML-based training data sampling method was showed to be effective, which is a useful technique when dealing with large aerial imagery datasets. DDR-Net also ran faster in inference.</p><p>In the future work, we plan to explore how DDR-Net and our sampling method function in other domains and a broader range of object detection tasks. We will improve DDR-Net and develop smart sampling and active learning techniques for object detection with limited data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>ChatGPT was used to revise the writing to improve the overall readability.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. DDR-Net architecture.</figDesc><graphic coords="2,318.25,50.54,238.51,149.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An example of feature map sizes from P3 to P6 of RetinaNet. The green grids represent anchor points at each feature map layer.</figDesc><graphic coords="3,55.24,50.54,238.51,179.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An example of estimating anchor sizes using k-means clustering on training data.</figDesc><graphic coords="3,324.53,300.10,225.95,196.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An example from seabirdwatch dataset showing different performance based on different training samples.</figDesc><graphic coords="5,61.52,50.55,225.93,222.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>generate unbalanced training sets, we only randomly selected training examples from a subset of the clusters outputted by the k-means clustering method, instead of selecting them from all clusters. Fig. 5 shows the results on two datasets, seabirdwatch and neill, In each figure, the first bar shows the distribution of clusters in the training set. The number of clusters was determined by adaptive k-means clustering method. There were 9 clusters for the seabirdwatch dataset and 3 clusters for the neill dataset. The next 10 bars shows training example distributions generated by the ML-based Image Sampling method in 10 trials. The variations are due to the randomness in our sampling method. The last two bars shows the training example distributions of two unbalanced training sets. The best F1 score and mAP values of DDR-Net model trained using each training set are shown as the red and blue curves, respectively. The results show that DDR-Net models trained using the ML-based Image Sampling method achieved consistent performances, much better than DDR-Net models trained using unbalanced training sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>V. CONCLUSIONS This paper presented DDR-Net, a new data-driven deep learning architecture based on RetinaNet, along with a MLbased training data sampling method. Experimental results on a large number of datasets of aerial images on birds</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) Results using the seabirdwatch dataset. (b) Results using the neill dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Distributions of training examples across clusters in the training sets generated by the proposed ML-based Image Sampling method and in the unbalanced training sets, and the corresponding performances (best F1 score and mAP values) of DDR-Net models trained using different training sets.</figDesc><graphic coords="7,52.45,227.92,251.03,144.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Performance comparison (best F1 score) of fine-tuned Bird Detector and DDR-Net models on nine datasets.</figDesc><graphic coords="7,61.52,440.07,225.94,92.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Identify Optimal Feature Map Size: Iterates through candidate feature map sizes, records annotation overlaps with different feature map sizes, and selects the best feature map size based on the score calculated in CalculateFeatureMapScore.procedure FINDOPTIMALFEATUREMAPSIZE(imageAnnotations, f eatureM apSizes)Input: imageAnnotations: A list of annotations in the image. f eatureM apSizes: A set of feature map sizes for each feature map. f eatureM apData ← empty dictionary for each level in f eatureM apSizes do for each annotation in imageAnnotations do Calculate Feature Map Score: Computes the score for each feature map size based on the number of overlaps on each feature map grid. Utilizes a weighting factor to measure grids with overlaps exceeding the number of anchors per grid, and a penalty factor to penalize grids with no matching annotations. A dictionary with feature map levels and overlapped annotations data.numAnchors: Number of anchors obtained from anchor estimation process. penalty_f actor ← 0.0001, weighting_f actor ← -1 metrics ← empty dictionary for each level, f reqs in f eatureM apData do desired_overlap_score ←</figDesc><table><row><cell cols="5">overlappedGrid ← FINDOVERLAPPEDGRID(annotation, level)</cell></row><row><cell cols="5">f eatureM apData[level][overlappedGrid] update by + 1</cell></row><row><cell cols="5">f eatureSizeScores ← CALCULATEFEATUREMAPSCORE(f eatureM apData, numAnchors)</cell></row><row><cell cols="5">optimalF eatureM apSize ← argmax(f eatureSizeScores)</cell></row><row><cell cols="3">return optimalF eatureM apSize</cell><cell></cell><cell></cell></row><row><cell cols="5">Algorithm 2 procedure CALCULATEFEATUREMAPSCORE(f eatureM apData, numAnchors)</cell></row><row><cell>Input:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">f eatureM apData: • In the usual way of calculating anchor regression, the</cell></row><row><cell cols="5">equations below are used to find the loss on width and</cell></row><row><cell cols="4">height (assuming best fit anchor boxes),</cell><cell></cell></row><row><cell>∆w = log</cell><cell>w w anchor</cell><cell>= log</cell><cell>50 64</cell><cell>≈ -0.358, (1)</cell></row><row><cell>∆h = log</cell><cell>h h anchor</cell><cell>= log</cell><cell>20 16</cell><cell>≈ 0.223,</cell></row></table><note><p>1≤key≤numAnchors f reqs[key] excessive_overlap_score ← key&gt;numAnchors f reqs[key] • (key -numAnchors) • weighting_f actor penalty ← number of grids without overlapped • penalty_f actor metrics[level] ← desired_overlap_score + excessive_overlap_score -penalty return metrics</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I SUMMARY</head><label>I</label><figDesc>OF THE PUBLIC DATASETS OF AERIAL IMAGES OF BIRDS USED IN OUR EXPERIMENTS<ref type="bibr" target="#b17">[17]</ref>.</figDesc><table><row><cell>Name</cell><cell cols="2">Location</cell><cell></cell><cell cols="3">No. of Training Annotations</cell><cell>No. of Test Anno.</cell><cell>GSD (cm)</cell></row><row><cell>Everglade</cell><cell cols="2">Florida, USA</cell><cell></cell><cell>50870</cell><cell></cell><cell></cell><cell>5648</cell><cell>1.01</cell></row><row><cell>mckellar</cell><cell cols="2">Canada</cell><cell></cell><cell>1537</cell><cell></cell><cell></cell><cell>82</cell><cell>1.33</cell></row><row><cell>michigan</cell><cell cols="3">Cape Cod, USA</cell><cell>40233</cell><cell></cell><cell></cell><cell>6199</cell><cell>0.91</cell></row><row><cell>monash</cell><cell cols="3">Melbourne, Australia</cell><cell>9846</cell><cell></cell><cell></cell><cell>327</cell><cell>1.51</cell></row><row><cell>neill</cell><cell cols="2">Utah, USA</cell><cell></cell><cell>39623</cell><cell></cell><cell></cell><cell>5119</cell><cell>0.88</cell></row><row><cell>newmexico</cell><cell cols="3">New Mexico, USA</cell><cell>4334</cell><cell></cell><cell></cell><cell>283</cell><cell>0.63</cell></row><row><cell>palmyra</cell><cell cols="3">Palmyra Atoll, South Pacific</cell><cell>1316</cell><cell></cell><cell></cell><cell>455</cell><cell>1.17</cell></row><row><cell>penguins</cell><cell cols="3">Antarctic Peninsula</cell><cell>2020</cell><cell></cell><cell></cell><cell>1505</cell><cell>3.32</cell></row><row><cell>pfeifer</cell><cell cols="3">South Shetland Islands, Antarctica</cell><cell>43010</cell><cell></cell><cell></cell><cell>2688</cell><cell>2.55</cell></row><row><cell>seabirdwatch</cell><cell cols="2">North Atlantic</cell><cell></cell><cell>124391</cell><cell></cell><cell></cell><cell>2362</cell><cell>2.59</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell></row><row><cell cols="8">PERFORMANCE COMPARISON OF LOCAL-ONLY RETINANET (I.E., BIRD DETECTOR) AND DDR-NET.</cell></row><row><cell>Dataset</cell><cell></cell><cell cols="2">Bird Detector [17]</cell><cell></cell><cell></cell><cell></cell><cell>DDR-Net</cell></row><row><cell></cell><cell>F1</cell><cell>mAp</cell><cell cols="2">Avg. Inference Time (s)</cell><cell>F1</cell><cell cols="2">mAp</cell><cell>Avg. Inference Time (s)</cell></row><row><cell>mckellar</cell><cell>0.57</cell><cell>45.5</cell><cell>0.343</cell><cell cols="2">0.67 (+17.5%)</cell><cell cols="2">63.76 (+40.0%)</cell><cell>0.302 (-12.0%)</cell></row><row><cell>michigan</cell><cell>0.66</cell><cell>55.43</cell><cell>0.341</cell><cell cols="2">0.82 (+24.2%)</cell><cell cols="2">82.1 (+48.0%)</cell><cell>0.281 (-17.6%)</cell></row><row><cell>monash</cell><cell>0.74</cell><cell>66.92</cell><cell>0.343</cell><cell cols="2">0.81 (+9.5%)</cell><cell cols="2">78.09 (+16.7%)</cell><cell>0.285 (-16.9%)</cell></row><row><cell>neill</cell><cell>0.91</cell><cell>89.49</cell><cell>0.356</cell><cell cols="2">0.92 (+1.1%)</cell><cell cols="2">90.3 (+0.9%)</cell><cell>0.275 (-22.8%)</cell></row><row><cell>newmexico</cell><cell>0.89</cell><cell>90.42</cell><cell>0.361</cell><cell cols="2">0.94 (+5.6%)</cell><cell cols="2">93.19 (+3.1%)</cell><cell>0.295 (-18.3%)</cell></row><row><cell>palmyra</cell><cell>0.79</cell><cell>77.21</cell><cell>0.340</cell><cell cols="2">0.79 (0.0%)</cell><cell cols="2">78.66 (+1.9%)</cell><cell>0.286 (-15.9%)</cell></row><row><cell>penguins</cell><cell>0.89</cell><cell>88.02</cell><cell>0.336</cell><cell cols="2">0.87(-2.2%)</cell><cell cols="2">87.03 (-1.1%)</cell><cell>0.281 (-16.4%)</cell></row><row><cell>pfeifer</cell><cell>0.86</cell><cell>81.02</cell><cell>0.341</cell><cell cols="2">0.88 (+2.3%)</cell><cell cols="2">89.25 (+10.2%)</cell><cell>0.279 (-18.2%)</cell></row><row><cell>seabirdwatch</cell><cell>0.72</cell><cell>57.49</cell><cell>0.356</cell><cell cols="2">0.83 (+15.3%)</cell><cell cols="2">75.27 (+31.0%)</cell><cell>0.257 (-27.8%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III AVERAGE</head><label>III</label><figDesc>GAINS OF DDR-NET MODELS AND P-VALUES OF PAIRED-T TEST RESULTS BETWEEN THE MAP VALUES OF FINE-TUNED BIRD DETECTORAND DDR-NET MODEL ON THE TEST SETS OF DIFFERENT DATASETS.</figDesc><table><row><cell>Dataset</cell><cell>Average mAP Gain (%)</cell><cell>P-value</cell></row><row><cell>mckellar</cell><cell>4.83</cell><cell>1.022 × 10 -2</cell></row><row><cell>michigan</cell><cell>4.97</cell><cell>8.075 × 10 -2</cell></row><row><cell>monash</cell><cell>39.19</cell><cell>3.306 × 10 -2</cell></row><row><cell>neill</cell><cell>0.8</cell><cell>1.451 × 10 -1</cell></row><row><cell>newmexico</cell><cell>3.07</cell><cell>5.616 × 10 -2</cell></row><row><cell>palmyra</cell><cell>38.41</cell><cell>5.873 × 10 -2</cell></row><row><cell>pfeifer</cell><cell>25.58</cell><cell>1.474 × 10 -3</cell></row><row><cell>seabirdwatch</cell><cell>11.8</cell><cell>7.893 × 10 -3</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Training Object Detection And Recognition CNN Models Using Data Augmentation</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Mas</forename><surname>Montserrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Allebach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
		<idno type="DOI">10.2352/issn.2470-1173.2017.10.imawm-163</idno>
	</analytic>
	<monogr>
		<title level="j">Electronic Imaging</title>
		<title level="j" type="abbrev">ei</title>
		<idno type="ISSNe">2470-1173</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="27" to="36" />
			<date type="published" when="2017-01-29">2017</date>
			<publisher>Society for Imaging Science &amp; Technology</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Augmentation for small object detection</title>
		<author>
			<persName><forename type="first">Mate</forename><surname>Kisantal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Murawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacek</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.5121/csit.2019.91713</idno>
		<idno type="arXiv">arXiv:1902.07296</idno>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Advances in Computing and Information Technology (ACITY 2019)</title>
		<imprint>
			<publisher>Aircc Publishing Corporation</publisher>
			<date type="published" when="2019-12-21">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Data Augmentation Strategies for Object Detection</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58583-9_34</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="566" to="583" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXVII 16</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.324</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Table 2: Performance comparison of fine-tuned models on training and testing datasets, illustrating their classification results.</title>
		<idno type="DOI">10.7717/peerjcs.2373/table-2</idno>
		<imprint>
			<date>null</date>
			<publisher>PeerJ</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Learning Methods for Tree Detection and Classification</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenduo</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><surname>Viegut</surname></persName>
		</author>
		<idno type="DOI">10.1109/cogmi56440.2022.00030</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE 4th International Conference on Cognitive Machine Intelligence (CogMI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-12">2022</date>
			<biblScope unit="page" from="148" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tree Detection and Species Classification in a Mixed Species Forest Using Unoccupied Aircraft System (UAS) RGB and Multispectral Imagery</title>
		<author>
			<persName><forename type="first">Poornima</forename><surname>Sivanandam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arko</forename><surname>Lucieer</surname></persName>
			<idno type="ORCID">0000-0002-9468-4516</idno>
		</author>
		<idno type="DOI">10.3390/rs14194963</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<title level="j" type="abbrev">Remote Sensing</title>
		<idno type="ISSNe">2072-4292</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">4963</biblScope>
			<date type="published" when="2022-10-05">2022</date>
			<publisher>MDPI AG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Individual tree detection and species classification of Amazonian palms using UAV images and deep learning</title>
		<author>
			<persName><forename type="first">Matheus</forename><forename type="middle">Pinheiro</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><forename type="middle">Roberti Alves De</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De Almeida</forename><surname>Papa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliano</forename><forename type="middle">Baldez Silva</forename><surname>Minervino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hudson</forename><forename type="middle">Franklin Pessoa</forename><surname>Veras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Formighieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><forename type="middle">Alexandre Nascimento</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcio</forename><forename type="middle">Aurélio Dantas</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evandro</forename><forename type="middle">Orfanó</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evandro</forename><forename type="middle">José Linhares</forename><surname>Ferreira</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.foreco.2020.118397</idno>
	</analytic>
	<monogr>
		<title level="j">Forest Ecology and Management</title>
		<title level="j" type="abbrev">Forest Ecology and Management</title>
		<idno type="ISSN">0378-1127</idno>
		<imprint>
			<biblScope unit="volume">475</biblScope>
			<biblScope unit="page">118397</biblScope>
			<date type="published" when="2020-11">2020</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A New GNN-Based Object Detection Method for Multiple Small Objects in Aerial Images</title>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Shang</surname></persName>
		</author>
		<idno type="DOI">10.1109/icis57766.2023.10210246</idno>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE/ACIS 23rd International Conference on Computer and Information Science (ICIS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023-06-23">2023</date>
			<biblScope unit="page" from="14" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">sUAS and Machine Learning Integration in Waterfowl Population Surveys</title>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><surname>Viegut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabeth</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Raedeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Sartwell</surname></persName>
		</author>
		<idno type="DOI">10.1109/ictai52525.2021.00084</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-11">2021</date>
			<biblScope unit="page" from="517" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimating Detection Probabilities of Waterfowl Broods From Ground‐Based Surveys</title>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">M</forename><surname>Pagano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">W</forename><surname>Arnold</surname></persName>
		</author>
		<idno type="DOI">10.2193/2007-524</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Wildlife Management</title>
		<title level="j" type="abbrev">J Wildl Manag</title>
		<idno type="ISSN">0022-541X</idno>
		<idno type="ISSNe">1937-2817</idno>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="686" to="694" />
			<date type="published" when="2009-07">2009</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data Augmentation for Object Detection: A Review</title>
		<author>
			<persName><forename type="first">Parvinder</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baljit</forename><forename type="middle">Singh</forename><surname>Khehra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Er.</forename><forename type="middle">Bhupinder Singh</forename><surname>Mavi</surname></persName>
		</author>
		<idno type="DOI">10.1109/mwscas47672.2021.9531849</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Midwest Symposium on Circuits and Systems (MWSCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-08-09">2021</date>
			<biblScope unit="page" from="537" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-Time Target Detection in Visual Sensing Environments Using Deep Transfer Learning and Improved Anchor Box Generation</title>
		<author>
			<persName><forename type="first">Zhenbo</forename><surname>Ren</surname></persName>
			<idno type="ORCID">0000-0001-7795-517X</idno>
		</author>
		<author>
			<persName><forename type="first">Edmund</forename><forename type="middle">Y</forename><surname>Lam</surname></persName>
			<idno type="ORCID">0000-0001-6268-950X</idno>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/access.2020.3032955</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<title level="j" type="abbrev">IEEE Access</title>
		<idno type="ISSNe">2169-3536</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="193512" to="193522" />
			<date type="published" when="2020">2020</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An Improved YOLO-v3Algorithm for Ship Detection in SAR Image Based on K-means++ with Focal Loss</title>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanni</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangxi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohui</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ciss57580.2022.9971239</idno>
	</analytic>
	<monogr>
		<title level="m">2022 3rd China International SAR Symposium (CISS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-11-02">2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FlexiNet: Fast and Accurate Vehicle Detection for Autonomous Vehicles</title>
		<author>
			<persName><forename type="first">Sabeeha</forename><surname>Mehtab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farah</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqi</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3484274.3484282</idno>
	</analytic>
	<monogr>
		<title level="m">2021 4th International Conference on Control and Computer Vision</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-08-13">2021</date>
			<biblScope unit="page" from="43" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive Weighted Multi-Level Fusion of Multi-Scale Features: A New Approach to Pedestrian Detection</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Xu</surname></persName>
			<idno type="ORCID">0000-0001-8546-5189</idno>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.3390/fi13020038</idno>
	</analytic>
	<monogr>
		<title level="j">Future Internet</title>
		<title level="j" type="abbrev">Future Internet</title>
		<idno type="ISSNe">1999-5903</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2021-02-02">2021</date>
			<publisher>MDPI AG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature fusion: parallel strategy vs. serial strategy</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing-Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Feng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0031-3203(02)00262-5</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<title level="j" type="abbrev">Pattern Recognition</title>
		<idno type="ISSN">0031-3203</idno>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1369" to="1381" />
			<date type="published" when="2003-06">2003</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A general deep learning model for bird detection in high‐resolution airborne imagery</title>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">G</forename><surname>Weinstein</surname></persName>
			<idno type="ORCID">0000-0002-2176-7935</idno>
		</author>
		<author>
			<persName><forename type="first">Lindsey</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vienna</forename><forename type="middle">R</forename><surname>Saccomanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashley</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Brush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenda</forename><surname>Yenni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><forename type="middle">E</forename><surname>Mckellar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Converse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Lippitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wegmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><forename type="middle">D</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><forename type="middle">J</forename><surname>Edney</surname></persName>
			<idno type="ORCID">0000-0003-1021-1533</idno>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Jessopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><forename type="middle">H</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Marchowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Senyondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Dotson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">P</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Frederick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K Morgan</forename><surname>Ernest</surname></persName>
		</author>
		<idno type="DOI">10.1002/eap.2694</idno>
	</analytic>
	<monogr>
		<title level="j">Ecological Applications</title>
		<title level="j" type="abbrev">Ecological Applications</title>
		<idno type="ISSN">1051-0761</idno>
		<idno type="ISSNe">1939-5582</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">e2694</biblScope>
			<date type="published" when="2022-08-10">2022</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection for multi-cluster data</title>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1145/1835804.1835848</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010-07-25">2010</date>
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised learning methods for data clustering</title>
		<author>
			<persName><forename type="first">Satish</forename><surname>Chander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vijaya</surname></persName>
		</author>
		<idno type="DOI">10.1016/b978-0-12-820601-0.00002-1</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence in Data Mining</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="41" to="64" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>

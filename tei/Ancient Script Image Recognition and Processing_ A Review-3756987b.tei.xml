<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ancient Script Image Recognition and Processing: A Review</title>
				<funder ref="#_jkpcvuN">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_wxYfYTN">
					<orgName type="full">Department of Science and Technology of Jilin Province, China</orgName>
				</funder>
				<funder ref="#_zXpCYGR">
					<orgName type="full">National Social Science Foundation of China</orgName>
				</funder>
				<funder ref="#_KnjeNnE">
					<orgName type="full">&quot;Paleography and Chinese Civilization Inheritance and Development Program&quot; Collaborative Innovation Platform</orgName>
				</funder>
				<funder ref="#_EU2PH95">
					<orgName type="full">Jilin University, China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-06-24">24 Jun 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaolei</forename><surname>Diao</surname></persName>
							<email>x.diao@qmul.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Cédric</forename><forename type="middle">M</forename><surname>John</surname></persName>
							<email>cedric.john@qmul.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Daqian</forename><surname>Shi</surname></persName>
							<email>d.shi@qmul.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Rite Bo</orgName>
								<orgName type="institution">Jilin University</orgName>
								<address>
									<settlement>Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Jilin University</orgName>
								<address>
									<settlement>Yanling Xiao, Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Jilin University</orgName>
								<address>
									<addrLine>Lida Shi</addrLine>
									<settlement>Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Jilin University</orgName>
								<address>
									<addrLine>Zhihan Zhou</addrLine>
									<settlement>Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">Jilin University</orgName>
								<address>
									<settlement>Hao Xu, Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">Jilin University</orgName>
								<address>
									<settlement>Chuntao Li, Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution">Jilin University</orgName>
								<address>
									<settlement>Xiongfeng Tang, Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="institution" key="instit1">Massimo Poesio</orgName>
								<orgName type="institution" key="instit2">Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<region>Queen</region>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff14">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ancient Script Image Recognition and Processing: A Review</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-06-24">24 Jun 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">8722046ED1999F8ED8CCD946651AA2AC</idno>
					<idno type="arXiv">arXiv:2506.19208v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-09-05T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ancient Scripts</term>
					<term>Optical Character Recognition</term>
					<term>Deep Learning</term>
					<term>Image Degradation</term>
					<term>Data Imbalance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ancient scripts, e.g., Egyptian hieroglyphs, Oracle Bone Inscriptions, and Ancient Greek inscriptions, serve as vital carriers of human civilization, embedding invaluable historical and cultural information. Automating ancient script image recognition has gained importance, enabling large-scale interpretation and advancing research in archaeology and digital humanities. With the rise of deep learning, this field has progressed rapidly, with numerous script-specific datasets and models proposed. While these scripts vary widely, spanning phonographic systems with limited glyphs to logographic systems with thousands of complex symbols, they share common challenges and methodological overlaps. Moreover, ancient scripts face unique challenges, including imbalanced data distribution and image degradation, which have driven the development of various dedicated methods. This survey provides a comprehensive review of ancient script image recognition methods. We begin by categorizing existing studies based on script types and analyzing respective recognition methods, highlighting both their differences and shared strategies. We then focus on challenges unique to ancient scripts, systematically examining their impact and reviewing recent solutions, including few-shot learning and noise-robust techniques.</p><p>Finally, we summarize current limitations and outline promising future directions. Our goal is to offer a structured, forward-looking perspective to support ongoing advancements in the recognition, interpretation, and decipherment of ancient scripts. CCS Concepts: • Computing methodologies → Computer vision; Image processing; • Applied computing → Optical character recognition; Document analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fragile artifacts and faded inscriptions house the understanding of past civilizations, with notorious examples including the Rosetta Stone and the Dead Sea Scrolls. The key to this knowledge is ancient scripts, such as ancient Greek inscriptions, Egyptian hieroglyphs, and Oracle Bone Inscriptions (OBIs). These archaic writing systems bear witness to 2 Diao et al. pivotal moments in human history and trace the lineage of civilization <ref type="bibr" target="#b104">[104]</ref>. By deciphering ancient scripts, scholars gain profound insights into the political, economic, and religious dimensions of past societies, highlighting civilization roots with impact into the modern world <ref type="bibr" target="#b8">[9]</ref>. Consequently, researching ancient scripts is of great importance not only for historical inquiry but also for the preservation of cultural heritage. As principal digital resources, ancient script images are key research objects for scientists <ref type="foot" target="#foot_0">1</ref> . Automated recognition and processing of these digitized images promises to enable the large-scale interpretation of historical scripts <ref type="bibr" target="#b54">[54]</ref>.</p><p>But ancient scripts are diverse with different characteristics and challenges <ref type="bibr" target="#b78">[78]</ref>. Based on the definition by linguists, widely studied ancient scripts can be categorized into (1) Phonographic scripts, such as Latin, Ancient Greek, and Sanskrit; and (2) Logographic scripts, such as OBIs, Sumerian Cuneiform, and Egyptian Hieroglyphs <ref type="bibr" target="#b98">[98]</ref>. These two types of scripts represent distinct approaches to linguistic encoding, resulting in shared data characteristics among scripts within the same type <ref type="bibr" target="#b87">[87]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates examples of images from both types of ancient scripts. Phonographic scripts encode segmental pronunciations into characters, each closely linked to the phonological structure of the language. Phonographic scripts generally contain a relatively small number of characters, characterized by simpler glyphs <ref type="bibr" target="#b62">[62]</ref>. Thus, characters need to be formed as words to express different meanings. On the other hand, logographic scripts focus on conveying semantics through individual characters, often employing symbols to represent objects or concepts <ref type="bibr" target="#b62">[62]</ref>. This results in a larger set of unique characters in logographic scripts, with intricate glyphs that represent complex meanings.</p><p>Recognizing ancient script images relies on manual analysis or computer vision-based image processing tools.</p><p>Experts manually analyze, classify, and interpret each character, making this process not only time-consuming but also prone to errors <ref type="bibr" target="#b146">[146]</ref>. Recent advances in artificial intelligence (AI), particularly optical character recognition (OCR) algorithms based on deep learning (DL), offer new opportunities to address these challenges <ref type="bibr" target="#b70">[70]</ref>. In recent years, numerous DL-and computer vision-based methods for ancient script recognition have emerged, aiming to perform classification, recognition, and interpretation tasks at a larger scale and with higher accuracy <ref type="bibr" target="#b2">[3]</ref>. Fig. <ref type="figure" target="#fig_1">2</ref> shows the number of publications and patents related to ancient script recognition from 2000 to 2024 <ref type="foot" target="#foot_1">2</ref> , revealing a pronounced upward trend, with a substantial number of scholarly papers published in the last five years. Given that many commonalities are shared in the recognition of different ancient scripts, we are motivated to undertake a more systematic review and synthesis of these studies, rather than studying each script in isolation <ref type="bibr" target="#b183">[183]</ref>. Some DL-based methods for recognizing ancient scripts have shown the feasibility of this approach; however, they also face challenges when applied to real-world scenarios. These challenges mainly arise from two aspects that distinguish ancient scripts from common modern scripts: (1) artifacts unearthed after thousands of years are often degraded or damaged, resulting in images containing significant real-world noise <ref type="bibr" target="#b147">[147]</ref>; and (2) the scarcity of unearthed artifacts results in an insufficient number of ancient script samples <ref type="bibr" target="#b38">[38]</ref>. These challenges degrade the quality of ancient script image datasets, making it difficult to meet the training requirements of DL-based models and ultimately leading to suboptimal recognition performance <ref type="bibr" target="#b121">[121]</ref>. Consequently, many studies have attempted various methods to overcome these problems. Currently, there remains a lack of reviews that specifically address the unique challenges associated with ancient script images.</p><p>In this paper, our objective is to conduct a systematic review of ancient script image recognition studies from multiple perspectives. Based on existing studies, we provide an in-depth discussion of the two major types of ancient scripts (logographic and phonographic), highlighting both the distinctions and interrelations between their respective image recognition approaches. Additionally, we provide an innovative review of two unique challenges specific to ancient script images: the presence of natural noise and the scarcity of available samples, discussing the resulting issues as well as corresponding solutions. This study also examines the limitations of current methodologies and explores potential directions for future research, offering researchers a broader perspective through meaningful observations.</p><p>Our contributions can be summarized as follows:</p><p>• We incorporate the latest methods for image recognition, providing an overview of recent advancements in ancient script recognition. We systematically present recognition approaches for 17 ancient scripts, primarily categorized as logographic and phonographic, discussing their interrelated research from a linguistic perspective.</p><p>• We conduct an in-depth examination of the application of few-shot learning methods in ancient script recognition, exploring strategies employed by related methods to effectively learn from very few samples using few-shot or zero-shot algorithms. We also introduce the issue of noise in ancient script image data, discussing the impact of different types of noise on recognition performance and presenting methods for accurate recognition in noisy images.</p><p>• We provide a visual summary in tabular form to compare our survey with previous ones. We highlight the areas addressed by previous studies and those <ref type="bibr">that</ref> have not yet been thoroughly discussed, establishing the unique contributions of this review. Additionally, we discuss the limitations of current methods and potential areas for future research.</p><p>The remainder of this survey is structured as follows: Section 2 introduces the types and characteristics of ancient scripts under consideration, clarifying how our review differs from existing surveys. Section 3 discusses methods tailored to recognizing logographic scripts, followed by Section 4, which deals with phonographic script recognition.</p><p>Section 5 examines the problem of few-shot learning, and Section 6 focuses on the presence of noise in ancient script images. Finally, Section 7 concludes the paper and proposes potential directions for future exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Revisiting Ancient Script Recognition</head><p>The taxonomy of ancient scripts is crucial for organizing and learning various recognition methods, as scripts within the same category often have similar writing features. In this section, we aim to systematically discuss the data features associated with these different script categories. We will explore the unique challenges that images of these ancient scripts pose to current recognition methods. Furthermore, we will outline the core motivations driving our survey and demonstrate how it distinguishes itself from existing surveys. By providing new perspectives and addressing critical gaps in this research domain, we aim to highlight the novel contributions of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Phonographic Scripts vs. Logographic Scripts</head><p>Based on linguistic principles and the visual features of script writing, widely studied ancient scripts can be broadly categorized into phonographic scripts and logographic scripts <ref type="bibr" target="#b78">[78,</ref><ref type="bibr" target="#b98">98]</ref>. Phonographic scripts are characterized by their representation of phonemes, the smallest units of sound in a language. These scripts typically employ smaller, simpler, and relatively uniform characters designed to systematically map sounds, as illustrated in Fig. <ref type="figure" target="#fig_18">1(a)</ref>. Representative examples include Latin cursive scripts, where characters represent individual sounds, and abugidas such as Sanskrit, where consonant-vowel combinations are encoded within single characters <ref type="bibr" target="#b61">[61]</ref>. This results in visual similarities across different phonographic scripts due to their shared focus on sound representation <ref type="bibr" target="#b165">[165]</ref>. Specifically, phonographic scripts exhibit the following common features:</p><p>• Small and similar characters: The characters in phonographic scripts, especially in handwritten forms, often bear visual similarity. For instance, letters like "e" and "f" in Latin cursive scripts are prone to confusion due to their shapes and minimal distinguishing features. This similarity increases recognition complexity when scripts are degraded or noisy.</p><p>• The recognition of words and characters: In phonographic writing systems, successfully recognizing individual characters is not necessarily considered the completion of the recognition task. This is because the semantics in these scripts are not carried by individual characters but rather by the combination of characters that form words. Consequently, corresponding recognition methods often also consider word-level recognition to achieve higher semantic accuracy.</p><p>• Hand-writing styles: Cursive or joined-up writing styles are frequently observed in phonographic scripts. Such connected strokes introduce noise and structural variations, making character segmentation and recognition particularly challenging. This issue is even more pronounced in ancient handwritten manuscripts, where stylistic variations and quality degradation are common.</p><p>Logographic scripts fundamentally differ from phonographic scripts in that their characters represent morphemes, the smallest units of meaning <ref type="bibr" target="#b34">[34]</ref>. These scripts are visually and structurally more complex, with characters often resembling intricate patterns rather than simple shapes <ref type="bibr" target="#b48">[48]</ref>. This distinction results in unique visual features for logographic scripts, setting them apart from phonographic scripts. Representative logographic scripts like OBIs and ancient Egyptian hieroglyphs can be found in Fig. <ref type="figure" target="#fig_0">1</ref>(b), where each character typically conveys a specific meaning or concept, representing morphemes rather than sounds. In particular, the following characteristics are observed in logographic scripts:</p><p>• Complex and Diverse Structures: Individual logographic characters are typically intricate, consisting of multiple strokes or components. This structural complexity makes them challenging to recognize, particularly in degraded or incomplete images.</p><p>• Sparse Character Set: Since each individual logographic character corresponds to a specific semantic meaning, the number of characters in such scripts is often significantly larger than in phonographic scripts. As a result, accurately recognizing each character presents a considerable challenge.</p><p>• Glyph Similarity: Logographic characters often contain a large number of reusable components, known as radicals or subcomponents, which carry specific semantic meanings and are frequently reused across different characters. This results in high visual similarity among many logographic characters. For instance, in Chinese, characters such as "材" and "林" share the common component "木", making it challenging to distinguish between these characters visually<ref type="foot" target="#foot_2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenges of Ancient Scripts Images</head><p>Ancient script images exhibit unique characteristics that pose significant challenges to conventional recognition methods. These challenges stem from two fundamental issues: limited sample availability and the real world noise in the images, both of which are deeply rooted in the historical and environmental conditions under which ancient scripts were created and preserved. One of the main challenges in ancient script recognition is the scarcity and imbalance of available data <ref type="bibr" target="#b90">[90]</ref>. Wars, natural disasters, and material degradation have significantly reduced the number of surviving manuscripts, inscriptions, and artifacts <ref type="bibr" target="#b95">[95]</ref>, limiting the total number of collectible samples. Moreover, character frequencies within these datasets are often highly uneven <ref type="bibr" target="#b176">[176]</ref>, with some characters appearing frequently while others occur only once or twice. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, both the OBI and Egyptian hieroglyph datasets<ref type="foot" target="#foot_3">4</ref> exhibit severe imbalance, where a few samples cover a large number of character classes. For example, the OBI dataset includes 2,806 distinct characters but only 12,170 samples, showing a pronounced long-tail distribution. This imbalance stems not only from historical losses but also from the domain-specific usage of ancient scripts <ref type="bibr" target="#b100">[100]</ref>, which often recorded specialized contexts like religious or ceremonial events, resulting in narrow vocabularies. These factors severely limit the availability of sufficient and diverse data, posing major challenges for recognition methods, particularly for data-driven approaches like machine learning, which rely on large, balanced datasets for optimal performance <ref type="bibr" target="#b39">[39]</ref>.</p><p>Compared to general text images, ancient script images are notably affected by degradation. This degradation arises from the artifacts' physical condition, preservation environment, and historical copying processes <ref type="bibr" target="#b87">[87]</ref>, often resulting in noise that obscures large portions of text and critical character features, making recognition highly challenging <ref type="bibr" target="#b57">[57]</ref>. Moreover, due to the long preservation period, the noise is typically heterogeneous, combining various types and levels <ref type="bibr" target="#b117">[117]</ref>, such as scratches, stains, erosion, and digitization artifacts, further complicating character recognition <ref type="bibr" target="#b195">[195]</ref>. One strategy is to preprocess images with denoising algorithms before recognition, but such methods have limitations. Unlike synthetic noise, which can be simulated and controlled, real ancient script noise is unpredictable and complex <ref type="bibr" target="#b148">[148]</ref>. The challenges of limited sample availability and the presence of degradation highlight the unique difficulties in recognizing ancient scripts. These observations underscore the necessity of developing specialized recognition algorithms to address the distinct challenges posed by ancient scripts. This also forms one of the key motivations for this survey, which aims to organize recognition and processing methods tailored to the specific issues inherent in ancient script images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surveys Year Published</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ancient Scripts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coverage of Scripts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods for Imbalanced Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods for Image Degradation</head><p>Shah and Badgujar <ref type="bibr" target="#b143">[143]</ref> 2013 ✓ Devnagari scripts ✓ Dineshkumar and Suganthi <ref type="bibr" target="#b41">[41]</ref> 2013 ✓ Sanskrit scripts Kataria and Jethva <ref type="bibr" target="#b84">[84]</ref> 2017 ✓ Arabic, Latin, Sanskrit Djaghbellou et al. <ref type="bibr" target="#b43">[43]</ref> 2021 Arabic scripts Hirugade et al. <ref type="bibr" target="#b70">[70]</ref> 2022 Devnagari scripts Krithiga et al. <ref type="bibr" target="#b87">[87]</ref> 2023 ✓ Tamil scripts Shen et al. <ref type="bibr" target="#b146">[146]</ref> 2023 ✓ Chinese scripts Chirimilla and Vardhan <ref type="bibr" target="#b27">[27]</ref> 2022 English, Indian ✓ Narang et al. <ref type="bibr" target="#b121">[121]</ref> 2020 ✓ 12 scripts without taxonomy Al-Taee et al. <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Distinction from Prior Work</head><p>Our main contribution is to offer, for the first time, a comprehensive survey that compares all existing ones on ancient script recognition. We demonstrate the distinction from existing surveys and reviews in Table <ref type="table" target="#tab_0">1</ref>. As can be observed, current surveys that specifically address ancient script recognition remain limited in number, and many are relatively outdated. Moreover, most existing surveys focus on a narrow subset of scripts or overlook the importance of script taxonomy in method analysis. This leads to a lack of connection between the inherent features of different scripts, limiting further discussions on recognition techniques. Furthermore, we observe that the unique challenges posed by ancient scripts are often underexplored in previous works, with little emphasis on the technical solutions needed to address them. In summary, the following key points outline the advancements of this survey compared to existing studies:</p><p>(1) Broader Coverage with Structured Taxonomy: We cover a wider range of ancient scripts and employ an efficient taxonomy to present recognition methods for different scripts;</p><p>(2) Discussion on Data Imbalance: We discuss the challenge of data imbalance in ancient scripts and review corresponding learning methods and strategies;</p><p>(3) Analysis of Degraded Images: We analyze recognition and processing methods for degraded ancient script images.</p><p>(4) Up-to-date Methods: We include more up-to-date computer vision methods specific for ancient scripts recognition;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Logographic Scripts</head><p>We categorize the ancient scripts into logographic and phonographic scripts. Building upon the linguistic family and geographic distribution of each script's origin and usage, we further organize and classify them in a more fine-grained manner. Fig. <ref type="figure" target="#fig_5">5</ref> presents the classification of all scripts along with their corresponding geographical distribution. The classification is grounded in the premise that the origin and usage of a script significantly influence the script's evolution.</p><p>Scripts that share a common origin or that were historically used in geographically adjacent regions tend to exhibit similar glyphs and writing features. Such a systematic organization provides a foundation for a structured exploration of the technical challenges and solutions associated with each script family. This section covers major ancient logographic scripts, including the Maya hieroglyphic scripts, ancient Egyptian hieroglyphic scripts, ancient Chinese scripts (oracle bone inscriptions, bronze inscriptions, Chu bamboo slips, and small seal scripts), ancient Mesopotamian cuneiform scripts, and the Proto-Elamite scripts from ancient Iran. These scripts, although diverse in geographic origin and stylistic form, share the fundamental features of combining visual symbolism with semantic encoding. Detailed descriptions of each script's writing characteristics and historical context are centralized here, whereas the subsequent subsections focus primarily on specific recognition technologies, dataset development, and modeling strategies relevant to each script.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Maya Hieroglyphic Scripts</head><p>Maya hieroglyphic scripts constitute a highly pictorial writing system that was extensively used to document core aspects of ancient Maya civilization (300 BCE -900 CE), including religious ceremonies, political affairs, and astronomical observations. These glyphs are distributed across present-day Mexico, Guatemala, and Honduras. As shown in Fig. <ref type="figure" target="#fig_6">6</ref>, characterized by a highly flexible graphemic structure and diverse writing orientations, Maya hieroglyphs pose considerable challenges for computational recognition and digitization. Early studies in this domain primarily focused on image retrieval. For instance, Roman et al. initiated the AJIMAYA project <ref type="bibr" target="#b136">[136]</ref>, which brought together archaeologists and computer scientists in a cross-disciplinary collaboration. The project collected high-resolution images and handdrawn sketches from several UNESCO World Heritage sites and conducted the first systematic evaluation of shape context descriptors for the recognition of syllabic Maya glyphs. Building on this foundation, Hu et al. proposed a glyph retrieval approach that combined shape similarity with contextual information <ref type="bibr" target="#b71">[71]</ref>. By modeling adjacent glyphs as a first-order Markov chain, their method enabled re-ranking of retrieval results based on co-occurrence probabilities, significantly improving the accuracy of visual matching.</p><p>To address the structural and stylistic complexity of Maya hieroglyphs, researchers explored interactive and visualization-based strategies to enhance the operability of image analysis workflows. In a subsequent study, Roman et al. expanded their glyph image dataset to include over 3,400 characters and introduced refinements to the HOOSC descriptor, making it more robust against open contours, variable stroke thicknesses, and intricate internal ornamentation. These improvements led to a 20% gain in retrieval performance <ref type="bibr" target="#b138">[138]</ref>. Additionally, they analyzed statistical models for glyph classification <ref type="bibr" target="#b137">[137]</ref>, further investigating co-occurrence statistics at the linguistic level to support the quantitative interpretation of ancient texts. Emphasizing the role of human-computer interaction, Hu et al. <ref type="bibr" target="#b72">[72]</ref> proposed a graph-based visualization interface in which nodes represent individual glyph images and edges encode visual similarity. This tool provided an intuitive means for scholars to explore complex hieroglyphic datasets and highlighted the value of human-in-the-loop approaches in ancient script recognition. In parallel, Bogacz et al. <ref type="bibr" target="#b16">[17]</ref> developed an image analysis pipeline that integrates multi-scale invariant filtering, random walk segmentation, and  5 as shown in Fig. <ref type="figure" target="#fig_6">6</ref>, and introduced a three-stage image processing pipeline involving shape matching, edge detection, and Levenshtein distance. Their method demonstrated strong performance in name identification and historical correlation, laying a practical foundation for applications such as museum-guided systems <ref type="bibr" target="#b44">[44]</ref>.</p><formula xml:id="formula_0">histogram</formula><p>Given the pictographic nature and stylistic diversity of hieroglyphic forms, conventional pattern-matching techniques often fall short. This has increasingly led to the adoption of deep learning approaches in the study of ancient scripts. achieving promising classification performance <ref type="bibr" target="#b12">[13]</ref>. They further extended their work to address the task of glyph segmentation, thereby enabling higher-level computational analyses such as symbol recognition, phonetic transliteration, and broader Egyptological applications <ref type="bibr" target="#b11">[12]</ref>. In a parallel effort, Guidi et al. segmentation, reading order control, post-processing, and HOG feature matching. Their system integrated the Gardiner sign list to significantly enhance automation and reading sequence accuracy, and stands as an early full-stack system aimed at "image-to-interpretation" tasks <ref type="bibr" target="#b49">[49]</ref>. In a more interactive direction, Plecher et al. developed ARsinoë, an augmented reality system that merges long short-term memory (LSTM) networks and 3D interaction design to support recognition and dynamic interaction with both printed and handwritten hieroglyphs <ref type="bibr" target="#b128">[128]</ref>. Further advancing this line of research, Sobhy et al. introduced an end-to-end hieroglyph interpretation framework that integrates R-CNN-based object detection, Siamese networks for few-shot classification, and character-level language modeling <ref type="bibr" target="#b153">[153]</ref>. [158] applied the scale-invariant feature transform (SIFT) algorithm to extract robust keypoints, enhancing character localization even in severely damaged inscriptions. By integrating a structured character database, their method achieved high recognition accuracy for incomplete glyphs. Chen et al. <ref type="bibr" target="#b25">[25]</ref> proposed an encoding-based recognition framework that transforms OBI images into compact matrices for efficient character identification. Their approach demonstrated high processing speed and accuracy on inscriptions from the Yin Ruins dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ancient Chinese Scripts</head><p>In recent years, the advancement of image processing, deep learning, and generative modeling has significantly improved both recognition accuracy and task complexity for OBIs <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b185">185]</ref>. Meng et al. <ref type="bibr" target="#b110">[110]</ref> were among the first to employ deep CNNs to extract visual features from OBIs. They applied data augmentation strategies, including rotation and brightness variation, to simulate real-world scenarios, achieving a recognition accuracy of 92.3%. Xing et al. <ref type="bibr" target="#b179">[179]</ref> conducted a comparative evaluation of several object detection architectures. YOLOv3 was selected as the optimal baseline, and further optimized through anchor box adjustments and loss function modifications tailored to the characteristics of oracle bone rubbings. To address issues of class imbalance and limited samples, Zhang et al. <ref type="bibr" target="#b194">[194]</ref> proposed a metric-learning-based CNN framework, using a triplet loss function to embed characters into a Euclidean space. This improved classification performance for rare and previously unseen character categories. At the same time, Huang et al. <ref type="bibr" target="#b75">[75]</ref> introduced the OBC306 dataset, which includes 309,551 samples across 306 OBI categories. This dataset has become a key benchmark for evaluating the performance of deep learning models. While architectures such as AlexNet and ResNet50 showed strong baseline performance, the study also highlighted the continued challenges of fine-grained classification.</p><p>In addition, Liu et al. <ref type="bibr" target="#b99">[99]</ref> investigated the use of lightweight CNN architectures such as SqueezeNet, which offered a favorable trade-off between computational efficiency and recognition performance. By applying extensive data augmentation, their model demonstrated enhanced accuracy in recognizing incomplete characters. For the task of OBI fragment reassembly, Zhang et al. <ref type="bibr" target="#b190">[190]</ref> introduced the OB-Rejoin dataset and developed a Siamese network-based method for fragment matching, achieving a reassembly accuracy of 80.9%, thereby extending the application of OBI recognition into the domain of physical restoration. In terms of script variant clustering, Liu et al. <ref type="bibr" target="#b96">[96]</ref> combined ResNet50 feature extraction with spectral clustering to automatically group visually similar glyph variants. Guo et al.</p><p>[67] proposed enhancements to the Inception-V3 architecture, incorporating rotation and scaling augmentations to improve robustness against noisy images. Wang et al. <ref type="bibr" target="#b170">[170]</ref> introduced the structure-texture separation network (STSN), which disentangles script structure from background noise. Their method demonstrated strong robustness in aligning printed and scanned OBIs, particularly under severe degradation. Xie et al. <ref type="bibr" target="#b178">[178]</ref> explored the use of diffusion models and proposed DiffOBI, a conditional diffusion-based framework for oracle bone script image generation. By introducing a dual-branch style control module and a hierarchical conditional modeling strategy, the method effectively generates high-quality oracle-style glyphs with strong stylistic consistency, enhancing the performance of data augmentation in few-shot oracle character recognition tasks. introduced a locally adaptive thresholding method, using an Effective Character Contour Length (ECCL) metric and multi-Gaussian fitting to determine optimal thresholds. Focusing on text detection, Jing et al. <ref type="bibr" target="#b80">[80]</ref> proposed an enhanced deep learning framework that integrates an IntraCL module, an adaptive feature pyramid, and a refined loss function, significantly improving detection accuracy in noisy and densely written slips.</p><p>3.3.4 Small Seal Scripts. Small seal script, also known as Zhuanshu, was the standardized script promoted during the Qin dynasty (around 200 BCE). It is characterized by rounded strokes, highly regular yet intricate structures, as shown in Fig. <ref type="figure" target="#fig_6">6</ref>. Xu et al. <ref type="bibr" target="#b180">[180]</ref> constructed the first large-scale Chinese seal dataset, which includes seals of various colors and difficulty levels. They developed a complete pipeline for detection, segmentation, and recognition, and established baseline models using YOLOv3 and related architectures, thereby extending the paradigm of traditional OCR research. Some studies focused on low-resource scenarios. Roy et al. <ref type="bibr" target="#b141">[141]</ref> proposed a structural hashing method that leverages spatial relationships between character pairs. Through hash table indexing and a voting mechanism, their approach achieved rotation and scale-invariant recognition, even when characters were partially missing, demonstrating robustness in complex visual conditions. Ou et al. <ref type="bibr" target="#b124">[124]</ref> combined HOG features with an SVM classifier, using Gaussian filtering and gamma correction to reduce noise.</p><p>Despite these advancements, the complexity of small seal script continues to pose significant challenges for recognition and restoration. To address these complexities, Tang et al. <ref type="bibr" target="#b163">[163]</ref> developed a lightweight CNN, ShuiNet-A, which integrates attention mechanisms to enhance the modeling of intricate stroke patterns. Wenjun et al. <ref type="bibr" target="#b174">[174]</ref> proposed EA-GAN, a restoration framework based on GANs, to recover missing small seal scripts in historical works. The model employs a dual-branch architecture to extract multi-scale contextual features and uses reference images to guide generation, effectively simulating human handwriting restoration and outperforming traditional interpolation-based methods. Zhou et al. <ref type="bibr" target="#b197">[197]</ref> addressed the zero-shot recognition by introducing a style-independent pictographic radical sequence learning framework. The model aligns small seal and traditional Chinese characters using GAN-based style normalization and employs a transformer to learn radical sequences. It demonstrated strong performance across three test sets, particularly in recognizing unseen characters and enabling cross-style translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ancient Mesopotamian Cuneiform Scripts</head><p>Cuneiform is one of the earliest known writing systems in human history (around 3000-600 BCE), originating in the region of Mesopotamia (present-day Iraq and parts of Syria) and remaining in use for over three millennia. It was typically impressed onto wet clay tablets using a reed stylus, producing wedge-shaped marks with well-defined strokes, as shown in Fig. <ref type="figure" target="#fig_6">6</ref>. Cuneiform is characterized by its strong directionality, abstract structural composition, and large inventory of symbols. In early computational studies, Bogacz et al. <ref type="bibr" target="#b14">[15]</ref>  To address the challenges posed by complex character structures and limited annotated data, Mara et al. constructed the HeiCuBeDa dataset <ref type="bibr" target="#b107">[107]</ref>, which comprises 3D models of 707 clay tablets with high-resolution surface features, six-view raster images, transliterations, phonetic interpretations, and metadata. This resource significantly lowers the barrier for applying computational methods to cuneiform analysis. Stotzner et al. <ref type="bibr" target="#b155">[155]</ref> proposed a CNN-based framework for symbol detection and classification and introduced an annotation mapping mechanism between rendered images and original photographs, confirming the utility of synthetic rendering for symbol localization and recognition.</p><p>ElShehaby et al. <ref type="bibr" target="#b50">[50]</ref> employed transfer learning using a VGG16-based model to develop a cuneiform symbol recognition system focused on scanned archaeological artifacts, particularly symbols from the Code of Hammurabi. With data augmentation, the system achieved competitive performance in symbol recognition.</p><p>In recent years, research on cuneiform recognition has gradually shifted from traditional image-based methods to multimodal fusion and interactive human-in-the-loop systems. Mousavi et al. <ref type="bibr" target="#b116">[116]</ref> proposed an integrated system that combines recognition, pronunciation, and translation, using preprocessing techniques such as median filtering, morphological operations, and edge blurring. The system achieved 92% recognition accuracy on both handwritten and inscribed texts and introduced a database of 1,500 annotated samples for old Persian cuneiform. Gordin et al.</p><p>[63] developed CuRe, an interactive platform supporting multi-stage recognition of hand-copied cuneiform, including character detection via Faster R-CNN, symbol classification with ResNet18, and a stroke-level segmentation module.</p><p>Additionally, Bogacz et al. <ref type="bibr" target="#b15">[16]</ref> provided a systematic review of recognition techniques across 2D images, vector graphics, and 3D scans, evaluating the applicability of curvature descriptors, keypoint-based modeling, GNNs, and GANs in cuneiform modeling workflows. Hagelskjaer et al. <ref type="bibr" target="#b68">[68]</ref> further introduced a large-scale point cloud network architecture tailored for classifying metadata of cuneiform tablets and proposed a maximum attention visualization method, enhancing model interpretability and achieving state-of-the-art results on benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Proto-Elamite Scripts</head><p>Proto-Elamite, one of the earliest known writing systems in ancient Iran, dates back to approximately 3100 BCE.</p><p>This script combines pictographic and abstract symbols, with a consistent linear arrangement. As shown in Fig. <ref type="figure" target="#fig_6">6</ref>, Proto-Elamite is considered a variant of cuneiform writing, but it differs notably from contemporaneous Mesopotamian cuneiform in several respects. While both employ wedge-shaped impressions, Proto-Elamite features more abstract, often less standardized symbols and exhibits a greater emphasis on alignment and tabular organization, especially in numerical and accounting contexts. Due to the substantial structural variability and frequent graphical mutations of its characters, a significant portion of the script remains undeciphered, posing considerable challenges for scholarly research. At the writing system level, Englund <ref type="bibr" target="#b51">[51]</ref>, under the framework of the Cuneiform Digital Library Initiative, systematically examined the structural characteristics of Proto-Elamite in terms of format, semantic organization, and numerical systems. He highlighted its higher degree of linearity relative to contemporary proto-cuneiform, along with its use of more static and diversified counting methods. Dahl <ref type="bibr" target="#b33">[33]</ref>, meanwhile, analyzed Proto-Elamite from the perspectives of script origin, regional distribution, and grammatical structure. Through stratigraphic analysis and internal textual features, Dahl also established a relative chronology for Proto-Elamite documentation.</p><p>In recent years, researchers have increasingly turned to computational approaches to analyze the structure of compositional graphemes within Proto-Elamite in order to uncover their combinatorial patterns and potential semantic rules. Born et al. <ref type="bibr" target="#b18">[18]</ref> employed a multimodal language model that jointly processes image sequences and associated labels, revealing that Proto-Elamite symbols exhibit a degree of compositional regularity. Their study demonstrated that image-driven models offer superior interpretability compared to text-only methods, thereby reducing annotator </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Phonographic Scripts</head><p>We cover nine major ancient phonographic scripts here: Sanskrit (including Devanagari), Tamil, ancient Greek inscriptions, Old Latin, Cyrillic, Hebrew, ancient North Arabian, and Ethiopic script. In view of their linguistic origin and historical usage as shown in Fig. <ref type="figure" target="#fig_5">5</ref>, they are classified into several subcategories: ancient Indian scripts, European alphabetic scripts, Hebrew script, and ancient Arabian scripts. Our aim is to reflect the historical transmission and evolution of the phonographic scripts' tradition, and to reorganize them according to their developmental trajectory.</p><p>Through this perspective, we will uncover shared patterns and methodological divergences in their recognition methods. We provide a detailed overview of the characteristics of these scripts, with the subsequent subsections focusing on computational approaches and recognition research specific to each script. and character boundary enhancement compared to traditional models such as KNN and SVM, making them well-suited for preprocessing tasks in Sanskrit manuscript recognition <ref type="bibr" target="#b88">[88]</ref>.</p><p>Devanagari Script is a widely used script in India and Nepal. The handwritten forms of Devanagari appear extensively in historical documents and exhibit significant stylistic differences compared to modern printed forms. These In the domain of writer identification for ancient Greek inscriptions, Panagopoulos et al. proposed an automated method that generates "platonic prototypes" of each letter's contour to facilitate writer attribution <ref type="bibr" target="#b125">[125]</ref>  To alleviate the dependency on large volumes of annotated data for OCR tasks, Gruber et al. proposed a font-based synthetic text generator, which can produce new characters and styles without requiring additional training <ref type="bibr" target="#b64">[64]</ref>.</p><p>Experiments conducted on the HKR datasets demonstrated that the incorporation of synthetic data significantly reduced both the character error rate (CER) and word error rate (WER). Furthermore, Polomac et al., leveraging the Transkribus platform, developed a series of HTR models tailored to 18th-century Serbian manuscripts <ref type="bibr" target="#b129">[129]</ref>. Supported by a small manually annotated training set, these models achieved substantial recognition accuracy and validated the potential for model transferability across stylistically similar manuscripts, thereby accelerating the digitization of the Serbian historical dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hebrew Scripts</head><p>Hebrew script, originating around 1,000 BCE, is one of the oldest Semitic writing systems in the world. After centuries of evolution, modern Hebrew remains the official script of Israel. However, historical documents exhibit extensive variations, frequent use of diacritical marks, and significant material degradation. Hebrew script exhibits an extensive system of diacritical marks, including vocalization dots and cantillation signs, which appear densely throughout the text, as illustrated in Fig. <ref type="figure" target="#fig_13">8</ref>. These features significantly increase the complexity of character detection and recognition for Hebrew scripts. In the domain of character detection, Rabaev et al. proposed a retrieval method for ancient Hebrew characters based on dynamic time warping. By incorporating a multi-model matching mechanism, their approach significantly improved retrieval precision under conditions of high character similarity and image degradation <ref type="bibr" target="#b132">[132]</ref>. proposed an efficient and font-independent algorithm for word and character segmentation <ref type="bibr" target="#b131">[131]</ref>. Their method utilizes vertical projection and interquartile range techniques for word segmentation, followed by projection contour analysis and character segmentation based on topological characteristics. Based on the APTI dataset, the algorithm achieved a word segmentation accuracy of 97.7% and a character segmentation accuracy of 97.5%. Due to its independence from specific font features, the method effectively reduces the number of ligatures and optimizes character boundaries, thus laying a solid foundation for subsequent OCR tasks.</p><p>In the more challenging domain of handwritten script recognition, researchers have increasingly adopted transformerbased approaches for better word-level recognition. The OCFormer model <ref type="bibr" target="#b112">[112]</ref>  proposed a new offline Arabic handwriting recognition approach based on transformer and sequence-to-sequence architectures <ref type="bibr" target="#b111">[111]</ref>. This method eliminates the need for external language models, improves training and inference efficiency, and achieves superior performance compared to previous state-of-the-art offline HTR systems when evaluated on the KHATT dataset, further expanding its potential applications in the digitization of Arabic historical manuscripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Ethiopic</head><p>Scripts. The Ethiopic script, originating from the South Arabian alphabet (around 800 BCE), has been used for centuries to write Ge'ez scripts 6 , and has subsequently been extended to modern languages such as Amharic and Tigrinya. The script is structurally complex, comprising a large number of basic and derived characters, with significant subtle variations between glyphs. Early recognition approaches focused on structural feature modeling, e.g.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assabie et al. proposed a method based on Direction Field Tensor to extract local linear features and construct character</head><p>prototype trees, thereby enhancing the robustness of character segmentation and matching <ref type="bibr" target="#b5">[6]</ref>. Further work combined structural and syntactic analysis to generate primitive relation trees and perform pattern matching against a knowledge base, achieving highly tolerant recognition in different fonts, sizes, and styles <ref type="bibr" target="#b6">[7]</ref>.</p><p>Addressing printed OCR tasks, Addis et al. introduced a recognition system based on single-line normalization and one-dimensional Bi-LSTM networks <ref type="bibr" target="#b0">[1]</ref>. Without relying on any external language models or post-processing, and trained on synthetic datasets covering Amharic, Ge'ez, and Tigrigna. Additionally, Deneke et al. conducted the first study on OCR for handwritten Ethiopic documents <ref type="bibr" target="#b37">[37]</ref>, constructing an expanded dataset and employing a fine-tuned  <ref type="bibr" target="#b1">[2]</ref> proposed an end-to-end trainable approach based on a CRNN architecture, achieving strong results on both synthetic and real-world datasets, thus establishing a benchmark for natural scene OCR of ethiopic scripts. Furthermore, Tadesse et al. <ref type="bibr" target="#b162">[162]</ref> proposed an architecture combining Gated</p><p>CNNs and Stacked Self-Attention modules, achieving CERs of 8.7% and 8.2% on the newly constructed HETD and HEWD datasets, respectively, further validating the applicability of advanced deep learning frameworks to handwritten Ethiopic text recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Recognition on Scarce and Imbalanced Datasets</head><p>One of the significant challenges in the study of ancient scripts is the prevalence of scarce and imbalanced datasets. On the one hand, the surviving corpus of manuscripts, inscriptions, and other artifacts bearing ancient writing is extremely limited <ref type="bibr" target="#b95">[95]</ref>, resulting in an overall paucity of textual samples. Furthermore, due to language use conventions and recording preference, the frequency of scripts within these datasets is highly skewed, showing a pronounced long-tailed distribution <ref type="bibr" target="#b176">[176]</ref>. This data imbalance imposes severe difficulties for ancient script recognition, especially for deep learning-based algorithms, whose success hinges on access to abundant and high-quality training data <ref type="bibr" target="#b12">[13]</ref>. In response to these challenges, existing approaches can be broadly categorized into two lines of research: recognition methods based on data augmentation and those that leverage external knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Recognition Based on Data Augmentation</head><p>The earliest augmentation pipelines relied on simple yet effective image-space edits. Oracle-50K <ref type="bibr" target="#b69">[69]</ref> (59,081 images, 2,668 classes) and SOC5519 <ref type="bibr" target="#b74">[74]</ref> (44,868 images, 5,491 classes) both contain hundreds of categories that appear fewer than ten times, while OBC306 <ref type="bibr" target="#b75">[75]</ref> exhibits 29 one-shot classes despite having more than 300,000 samples overall. On such corpora, the Orc-Bert Augmentor first rasterizes each glyph into a sequence of stroke vectors, then perturbs those vectors with token-level masking and swapping during a self-supervised BERT pre-training phase; the reconstructed strokes are finally re-rendered to image form. This two-step procedure adds 5-10 synthetic mates to every real sample and provides roughly 4% absolute accuracy gains for ResNet-50 and DenseNet-161 in all head and tail boxes <ref type="bibr" target="#b69">[69]</ref>. A comparable strategy guides GlyphNet: starting from only 4,310 grayscale images in dataset D1 and 1,310 RGB images in dataset D2, the authors apply ±10°rotation, 0.95-1.05 scaling and horizontal flips, expanding the training pool to 7,340 samples; the purpose-built ten-layer CNN then surpasses ResNet-50 by 6 pp top-1 accuracy on a 40-class merge set <ref type="bibr" target="#b12">[13]</ref>. For Chinese bamboo-slip characters, ACCAN combines erosion, dilation, and elastic shearing with multi-scale attention, boosting tail recall by 12 pp over a plain transformer baseline <ref type="bibr" target="#b176">[176]</ref>.</p><p>Later studies focused on tailoring augmentation to the long-tail distribution itself. Repatch cuts a discriminative patch from a rare-class image and pastes it onto a frequent-class canvas, while TailMix chooses the Mixup ratio inversely proportional to class prior; together they lift Oracle-20K <ref type="bibr" target="#b89">[89]</ref> tail accuracy (classes with &lt;60 samples) from 18% to 46% without hurting head performance <ref type="bibr" target="#b90">[90]</ref>. Zhang and colleagues go one step further: class activation maps isolate glyph foreground, then only that region is randomly warped or colour-jittered, whereas the background remains untouched.</p><p>When combined with re-weighting, the method reduces false positives on visually similar bronze inscriptions by nearly one-third <ref type="bibr" target="#b193">[193]</ref>.</p><p>Generative augmentation now dominates large-scale efforts. Training StyleGAN2 on the AL-ALL corpus, which contains merely 30-40 exemplars for many Greek letters, produces photo-realistic 128×128 crops whose Fréchet Inception Distance is below 48. Injecting just 100 synthetic instances per tail class raises mean per-class accuracy by 8-12% on the AL-SYNTH benchmark <ref type="bibr" target="#b159">[159,</ref><ref type="bibr" target="#b161">161]</ref>. For degraded Kuzushiji manuscripts, Kaneko et al. adopt a denoising-diffusion restoration model: a binary mask restricts reverse-diffusion noise prediction to the missing region, so the model does not hallucinate global artefacts and requires no fine-tuning on the target page <ref type="bibr" target="#b81">[81]</ref>. Similarly, Gui et al. <ref type="bibr" target="#b65">[65]</ref> introduce a diffusion-based method to generate handwritten Chinese characters from printed fonts using few real samples.</p><p>Experimental results demonstrate that the synthetic data achieves recognition accuracy close to real samples, showing strong potential for handwritten OCR under data-scarce conditions.</p><p>A complementary research line embeds augmentation into the recognition engine itself. Prototype Calibration trains a generator conditioned on a stroke template and an image encoder that shares a metric space with those templates; at test time the generator hallucinates just enough samples to shrink the distance between unseen glyphs and their nearest prototype, giving a 5% edge on handwritten and street-scene Chinese datasets without any additional back-propagation <ref type="bibr" target="#b4">[5]</ref>. Stroke Similarity Network (SSN) removes the need for synthetic pixels entirely: a multi-branch backbone extracts global and local cues, and the SSCL loss pushes similar pairs closer while pulling dissimilar ones apart by at least a 0.5 margin; on the extremely skewed HWAYI set SSN doubles tail-class F1 over a plain triplet-loss baseline <ref type="bibr" target="#b100">[100]</ref>. Kindred Siamese designs achieve 82% zero-shot accuracy on Chars74K English letters with contrastive loss <ref type="bibr" target="#b47">[47]</ref>, 68% harmonic mean on unseen Bangla sign-language letters <ref type="bibr" target="#b122">[122]</ref>, and outperform deeper ResNet-50 models in a five-period Chinese character evolution study <ref type="bibr" target="#b168">[168]</ref>. In essence, these metric-learning approaches treat the embedding geometry itself as a form of implicit augmentation, ensuring that every hard-won sample, however few, exerts maximal influence during both training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Recognition Leveraging External Knowledge</head><p>The earliest successes in knowledge-guided recognition came from treating a character as a sequence of radicals and strokes defined by the Unicode ideographic description sequence (IDS). Radical Analysis Network (RAN) pairs a VGG-style CNN encoder with an RNN decoder, the two bridged by an attention module. The encoder delivers visual features, while the decoder outputs the corresponding radical string, shrinking the label space from 3,755 Hanzi to just 214 radicals and lifting zero-shot top-1 accuracy on unseen printed characters to 62%, more than twice that of a whole-glyph softmax <ref type="bibr" target="#b192">[192]</ref>. SIR-ZSCCR revisits the same idea from an information-theoretic angle: each radical is weighted by self-information, so frequent, low-value strokes are down-weighted. Two variants are offered, sequence matching and attribute embedding, both evaluated on the dataset HWDB <ref type="bibr" target="#b182">[182]</ref>, where the embedding model improves RAN by 4% on the unseen split and is markedly faster because the radical weights are pre-computed <ref type="bibr" target="#b102">[102]</ref>. SideNet introduced dual branches that learn radical knowledge and visual shape jointly, then fuse them through similarity guidance to handle handwriting, artistic fonts, street scene text, and ancient scripts in one model <ref type="bibr" target="#b92">[92]</ref>. STAR dives one level deeper, training parallel encoder-decoder pairs for strokes and radicals; a correlation loss aligns the two spaces, and an inference-time stroke-selection module prunes implausible candidates. On datasets Synth-Art and CTW, it raises radical-level recall from 71% to 88%, and still matches whole-glyph CNNs on seen classes <ref type="bibr" target="#b187">[187]</ref>. At the finest granularity, SAE pre-trains on 90 canonical stroke types from the Hanzi-Writer engine and reconstructs stroke sequences; when fine-tuned on the dataset HWDB it beats RAN by 6 pp on zero-shot characters <ref type="bibr" target="#b26">[26]</ref>. A dual-task GAN addresses stylistic variance: one branch translates small-seal glyphs to traditional Chinese, the other learns a Transformer that decodes style-independent radical sequences; on the most difficult Test-III (unseen radical + unseen style), it exceeds ResNet by 10% <ref type="bibr" target="#b197">[197]</ref>.</p><p>A second research stream models scripts as nodes in explicit graphs, or knowledge graphs. RZCR begins with a Radical Information Extractor that proposes up to fifteen candidate radicals and their spatial relations; a Graph Convolution-based Knowledge Reasoner then traverses a Character Knowledge Graph containing 5,492 nodes and 18,000 edges. On the tail half of SOC5519, accuracy jumps from 22% to 41% <ref type="bibr" target="#b39">[39]</ref>. Unified Character Recogniser (UCR) builds on this by attaching a confidence predictor and enforcing dual supervision between its character and radical heads, reaching 79.4% closed-set and 52.3% zero-shot accuracy on Oracle-FS <ref type="bibr" target="#b91">[91]</ref>. JRED goes lighter: every radical is a 200-dimensional learnable detector that slides over feature maps to yield radical attributes on the fly, giving state-of-the-art scores on Oracle-AYNU and Oracle-FS without any external graph storage <ref type="bibr" target="#b103">[103]</ref>. FaRE makes each TrueType radical rendered to 64 × 64 pixels, embedded by a frozen ResNet18 and binarised into a 64-bit code; Hamming search then supplies near-instant look-ups and adds 3% to zero-shot accuracy on ICDAR2013 <ref type="bibr" target="#b188">[188]</ref>. All these models depend on newly annotated corpora: ACCID provides 2,892 character classes with 595 radical labels and bounding boxes, OracleRC normalises 2,005 classes into 202 radicals and 14 spatial relations, while OBI-100 and OBI125 extend coverage to bronze, Chu, Qin, and seal scripts.</p><p>Hierarchical approaches exploit a tree rather than a graph structure. RSST first predicts a radical string, then decomposes every radical into its stroke list, finally encoding the result as a radical-structured stroke tree; weighted edit distance drives dictionary lookup. The system resists 30% Gaussian blur, 20% random occlusion, and a synthetic domain shift, still improving top-1 by 1.7-7.6% over one-stage baselines <ref type="bibr" target="#b184">[184]</ref>. VGTS generalises the idea to text spotting: dual spatial attention localises discriminative regions, geometric matching aligns query and support images, and a torus loss (margin = 0.1) shapes the embedding; on the low-resource Dongba and Tripitaka Koreana sets, one-shot spotting accuracy reaches 86% and 78% respectively <ref type="bibr" target="#b73">[73]</ref>. In Egyptian hieroglyphics, an end-to-end pipeline pairs R-CNN detection with a Siamese glyph classifier and a language model, delivering 95% mean precision and a BLEU of 59.2 on D1/D2/D and Morris-Franken-4210 <ref type="bibr" target="#b153">[153]</ref>. DeepScribe applies a 141-sign Elamite taxonomy to the highly Zipfian PFA archive: RetinaNet finds signs, ResNet recognises them, and the system reaches 0.84 mAP despite the top-50 signs covering 86% of data <ref type="bibr" target="#b175">[175]</ref>.</p><p>External-knowledge methods also thrive on phonographic scripts. Bengali word recognition uses a 13-attribute signature matrix that marks key stroke formations; a zero-shot model trained on 40 seen words attains 68% harmonic mean on six unseen words <ref type="bibr" target="#b24">[24]</ref>. For medieval Latin, a similar attribute scheme plus deep features yields 56.9% on 50</p><p>unseen classes <ref type="bibr" target="#b23">[23]</ref>. VGTS exploit dual spatial attention localises discriminative regions, geometric matching aligns query and support images, and a torus loss shapes the embedding; on the low-resource Dongba and Tripitaka Koreana sets, one-shot spotting accuracy reaches 86% and 78% respectively <ref type="bibr" target="#b73">[73]</ref>. PHOSC-CTC extends the attribute idea with phoneme tags: trained on MFU, GW, and IAM, it improves zero-shot word accuracy by 9% over Pho(SC)Net <ref type="bibr" target="#b13">[14]</ref>. Across the spectrum, from IDS strings to knowledge graphs and hierarchical trees, structural priors act as scaffolding that compensates for missing data, letting modern vision models decipher scripts preserved in only a handful of fragile artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Recognition on Degraded Images</head><p>During their historical transmission, ancient script artifacts, due to the inherent fragility of their materials and various environmental factors, often suffer damage such as wear, fractures, and corrosion <ref type="bibr" target="#b87">[87]</ref>. These damages not only lead to partially missing scripts, deformed strokes, and mottled backgrounds in the images of ancient scripts, but also inevitably introduce noise into their digitized representations. Such noise includes disruptions of script outlines, as well as spurious strokes arising from stains, cracks, or other non-script elements. Because of the pervasive presence of such noise, it is often difficult to distinguish genuine strokes from interfering artifacts, thus greatly increasing the complexity of recognition. Compared with noise found in general images, noise in ancient script images exhibits several distinctive features <ref type="bibr" target="#b195">[195]</ref>. First, multiple types of noise tend to coexist, including randomly distributed spots and scratches, large areas of damage, and residual voids in text rubbings. Second, these noises frequently resemble the original strokes so closely that they can easily be misidentified as part of the script, significantly complicating both noise reduction and feature extraction <ref type="bibr" target="#b148">[148]</ref>. In this section, we offer a detailed discussion of the two principal solution strategies to which recent studies have gravitated: (1) noisy script image recognition: developing robust recognition algorithms to directly handle noisy images, and (2) script image denoising: performing image denoising prior to recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Noisy Script Image Recognition</head><p>Wang and Deng introduced the Oracle-MNIST dataset, which comprises 30,222 grayscale images of ancient Chinese scripts in 10 categories most affected by noise interference. They compared several traditional machine learning classifiers, such as MLPClassifier and SGDClassifier, on this dataset <ref type="bibr" target="#b169">[169]</ref>, demonstrating that noise significantly challenges these algorithms. For recognizing such noisy ancient script images, early work applied AlexNet for script classification and improved model generalization through rotations, noise injection, and brightness adjustments <ref type="bibr" target="#b110">[110]</ref>,</p><p>aiming to improve recognition accuracy by parameter tuning. A later study embraced a similar data-driven perspective but focused on more robust detection, redesigning the recognition process using k-means clustering and introducing a spatial pyramid block to suppress noise; This approach outperformed the mainstream detectors in terms of the F-score by adding typical noise to clean images for training <ref type="bibr" target="#b97">[97]</ref>. Similarly, the data augmentation-based Diff-Oracle integrated a style encoder and content encoder to generate novel OBI samples with high visual fidelity, ultimately improving recognition accuracy in complex scenes <ref type="bibr" target="#b178">[178]</ref>. for the recognition of ancient stele inscriptions, which achieved promising precision in the noisy OBC306 dataset <ref type="bibr" target="#b67">[67]</ref>. Other approaches used non-end-to-end detection and classification strategies: for example, researchers explored YOLOv4 for detection with positive results, although complex noise conditions remained challenging <ref type="bibr" target="#b172">[172]</ref>. In practical detection upgrades, modifications to the YOLOv8 architecture-including an added small object detection head, a revised loss function, and attention modules-further boosted performance under noisy conditions <ref type="bibr" target="#b196">[196]</ref>.</p><p>In cross-scene recognition, where rubbings and excavated artifacts often present complex backgrounds, <ref type="bibr">Gao et</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Script Image Denoising</head><p>Another strategy for handling noisy ancient script images involves first improving image quality through denoising methods before proceeding with alignment and recognition. Early approaches frequently employed classical filtering techniques: Shi <ref type="bibr" target="#b150">[150]</ref> used a K-SVD-based method to preserve structural information by fusing low-and high-frequency images for denoising, then eliminated isolated "ant-like" noise at the binary stage. In a related endeavor, Shi et al.</p><p>[149] integrated multiple smoothing filters (ranging from guided filtering to Otsu thresholding) to remove random and blocky noise in classical calligraphy documents. These ideas are similar to the approach of Nair <ref type="bibr" target="#b118">[118,</ref><ref type="bibr" target="#b119">119]</ref>, who explored a multistage framework-including sharpening, background subtraction, and flood filling-to address uneven illumination in stone-carved Kannada inscriptions and ancient Malayalam manuscripts. Huang et al. <ref type="bibr" target="#b76">[76]</ref> offered a broad comparative study of denoising methods (e.g., anisotropic diffusion, total variation, and bilateral filtering), evaluating their performance on rubbings using metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM).</p><p>Meanwhile, some researchers tried to combine filtering with machine learning. Karthikeyan et al. <ref type="bibr" target="#b83">[83]</ref> demonstrated the applicability of these concepts in ancient Tamil stone carvings by combining a Lion optimization algorithm with transfer learning. They automatically adjust brightness and contrast, then use binarization alongside median filtering and Gaussian blurring to remove noise. Similar domain-transfer techniques have been applied to OBI recognition, where Wang et al. <ref type="bibr" target="#b170">[170]</ref> proposed a new STSN framework capable of disentangling structural and textural features under an unsupervised setting to mitigate severe noise. Traditional feature descriptors, such as HOG, also remain pertinent for specific tasks: Ou et al. <ref type="bibr" target="#b124">[124]</ref> applied HOG with SVM and Gaussian filtering to denoise partially blurred or incomplete small seal scripts. Some works continue to rely on classical image processing steps, as seen in Meng's Hough-transform-based pipeline <ref type="bibr" target="#b109">[109]</ref>, which employs Gaussian smoothing and Otsu thresholding to filter noise and isolate clear script boundaries. However, these methods often address only minor noise in images and provide limited improvement regarding stroke-like artifacts that fundamentally impair recognition.</p><p>Beyond filtering, other researchers focus on superior segmentation techniques or lightweight neural networks to separate target scripts from complex backgrounds. Sun et al. <ref type="bibr" target="#b158">[158]</ref> mitigate persistent noise in OBI rubbings by extracting SIFT key points to identify scripts corroded by scratches or breakage, and they also adopt a ResNet50-based method to handle handwritten OBI data. Zhang <ref type="bibr" target="#b191">[191]</ref> model denoising as a generative modeling task, embedding residual dense blocks and introducing novel noise definitions to train with unpaired data. Huang <ref type="bibr" target="#b77">[77]</ref> relies similarly on residual learning in a feedforward denoising CNN, followed by local adaptive thresholding for binary segmentation.</p><p>Yue <ref type="bibr" target="#b186">[186]</ref> employs morphological segmentation to preprocess newly discovered OBI documents, then applies a small neural network to remove erroneous segments before classifying the resulting data using a dynamic k-means approach.</p><p>Yalin <ref type="bibr" target="#b181">[181]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Despite substantial typological and visual differences, a number of recurring challenges and methodological patterns emerge across studies. One shared consensus among researchers is that ancient script recognition cannot simply adopt off-the-shelf OCR tools designed for modern scripts. Instead, tailored approaches are often required due to the unique characteristics of ancient datasets.</p><p>Interestingly, our review reveals a degree of methodological convergence among scripts with similar visual features.</p><p>For instance, Latin, Greek, and Cyrillic texts frequently raise problems related to cursive writing, ligature detection, and word-level modeling. These scripts share stylistic traits such as fluid lineation and glyph fusion in handwritten manuscripts. As a result, deep learning solutions often adopt similar techniques such as sequence-to-sequence models, CTC-based recognizers, or word-embedding-assisted pipelines. This cross-script generalization suggests that organizing research by script type, rather than by language alone, can yield a more principled understanding of OCR in historical contexts.</p><p>A key divergence between logographic and phonographic scripts lies in the role of linguistic context modeling. For phonographic systems, language modeling is essential: the presence of compositional phoneme-grapheme correspondences and grammar-driven word structures makes contextual reasoning central to both character-and word-level recognition. Systems such as CTC-based recognizers or language models are heavily relied upon in these tasks, especially when dealing with noisy manuscripts or ambiguous ligatures. In logographic scripts, however, context is often secondary to visual decomposition. Although contextual cues (e.g., surrounding characters or line structure) may assist in disambiguation, the core recognition task typically centers on the visual integrity of individual glyphs, particularly their radical-stroke configurations.</p><p>The difference in category granularity further differentiates modeling strategies. Logographic scripts often comprise thousands of distinct characters, many of which appear infrequently. This results in a typical "large-vocabulary, fewshot" setting, in which the model must learn to distinguish subtle structural differences with limited training data.</p><p>While this sparsity poses challenges, the compositional richness of logographs also provides opportunities for few-shot or even zero-shot recognition via structural modeling. Historical evolution and stylistic variation present challenges across both script types, but manifest differently. In phonographic systems like Latin or Greek, character shapes can differ dramatically across centuries, e.g., Old Latin scripts each exhibit distinct letterforms. Similarly, Indic scripts such as Sanskrit and Devanagari evolve through layered ligatures and regional variants. Logographic systems, too, display diachronic variation, but typically through the recombination of constituent radicals or structural transformations. The evolution of scripts like OBI, Small Seal, or Maya glyphs reflects this tendency. These patterns imply that robust OCR systems must not only model glyph shapes but also their permissible transformations over time, highlighting the need for temporally adaptive or style-invariant models. We also observe that emerging recognition frameworks increasingly adopt hybrid solutions, integrating visual attention, structural parsing, and contextual modeling within a unified architecture. This convergence suggests that future research may benefit from cross-script transfer learning and the development of universal representations that jointly account for linguistic form and visual structure.</p><p>Imbalanced and scarce scripts recognition. One of the challenges in ancient script recognition lies in the highly imbalanced distribution of character categories across datasets. Common issues include severe overfitting to head classes, low recall for tail classes, and overall evaluation metrics being disproportionately dominated by frequently occurring categories. Existing approaches to handling these problems can be broadly divided into two categories.</p><p>The first category focuses on improving the availability of data for low-frequency classes by increasing the image level. These techniques typically apply sampling strategies and conventional geometric transformations such as scaling, rotation, and flipping to expand the dataset. While effective to some extent in mitigating data sparsity, these methods often fail to generalize well in real-world scenarios, where models still suffer from overfitting and poor performance on truly rare samples. Nevertheless, certain methods show promising potential, particularly those designed specifically for long-tailed distributions, such as Mixup-based techniques like Repatch and TailMix, as well as generative approaches based on GANs or diffusion models that synthesize rare character instances.</p><p>The second category aims to improve recognition performance by leveraging external knowledge, thereby enabling few-shot or even zero-shot recognition of ancient scripts. These methods rely on structured prior information such as predefined character decomposition (e.g., IDS expressions, dictionaries, or knowledge graphs). By identifying and matching subcomponents, such as radicals or strokes, against these resources, models can infer the identity of previously unseen characters. Structural decomposition combined with graph-based reasoning has proven especially effective for tail classes. However, the limitation of these methods lies in their dependency on high-quality, pre-annotated external knowledge, which may not always be available or complete for all script systems.</p><p>Few-shot recognition is particularly prevalent in logographic writing systems such as Chinese, Egyptian hieroglyphs, or ancient scripts with extensive character vocabularies and significant structural complexity. In these cases, both augmentation-based and knowledge-driven methods are often required in tandem. By contrast, phonographic scripts such as Latin, Bangla, or Japanese Kana typically contain fewer character classes and exhibit more balanced data distributions. As a result, few-shot recognition in these systems tends to rely primarily on image augmentation, since their characters often represent atomic phonemes that cannot be further decomposed.</p><p>It is also worth noting that many existing datasets do not reflect the true distributional characteristics of historical corpora. Instead, they are often constructed by intentionally selecting balanced or artificially diversified character sets, which may yield favorable benchmark results, but limit the real-world applicability of the resulting models. This mismatch between data construction and application scenario significantly undermines the generalizability of trained models. Future research must therefore prioritize the development of standardized datasets that better reflect the statistical and structural realities of ancient writing systems. Finally, zero-shot recognition stands out as a crucial future direction. Its ability to recognize unseen characters is of particular importance for the decipherment and interpretation of unfamiliar or previously undeciphered scripts.</p><p>Noisy scripts recognition. In the recognition of ancient script images, image degradation poses an unavoidable and highly challenging obstacle. We systematically discuss recognition and processing methods for noisy ancient character images and classify them into two broad categories.</p><p>The first category includes approaches that attempt direct recognition without prior denoising. In general, noise severely hampers the ability of a model to extract meaningful features and can even mislead the network into interpreting noise artifacts as legitimate strokes. Therefore, such recognition models must place greater emphasis on enhancing their discriminative ability in the presence of degradation. These "noise-robust recognition" strategies typically rely on large-scale pretraining, additional supervised annotations, or the integration of external domain knowledge to strengthen the model's understanding of complex and degraded inputs. Although these methods can achieve impressive performance when abundant high-quality data is available, their practical adoption is constrained by the need for extensive labeled data and a trade-off between generalization and computational efficiency.</p><p>In most current research practices, especially under limited data regimes, performing denoising as a preprocessing step remains a more pragmatic approach. However, denoising ancient script images faces even more challenges. The noise in such images is often highly heterogeneous and arises from ink degradation, erosion, document aging, and complex backgrounds, making it difficult to simulate using synthetic noise for training purposes. These "compound noises" are often multi-source, accumulated over centuries, and not easily removed using standard filtering techniques.</p><p>Although classical filters were widely applied in early studies, they have proven insufficient to effectively restore the integrity of severely degraded ancient script images.</p><p>Deep-learning-based denoising models, such as diffusion models and GANs, have emerged, offering stronger image modeling capacity and better domain adaptability. These models are often capable of performing transfer learning across different writing systems and script styles, making them increasingly popular for handling noisy ancient texts.</p><p>Evaluated on various ancient languages and scripts, such models demonstrate significant improvements in readability and recognition accuracy while preserving the original structural traits of the characters. However, key limitations remain. Because most denoising models operate on pixel-level reconstructions without true semantic awareness, they often fail to distinguish real strokes from stroke-like noise. This results in two common issues: genuine character components may be mistakenly removed, or background noise may be preserved as false strokes. These limitations underscore the need for future denoising algorithms to incorporate richer linguistic and structural priors, such as stroke order, radical composition, and even syntactic rules, to enable semantically informed and structure-aware denoising.</p><p>It should be emphasized that the benefits of effective denoising extend beyond automated recognition systems. The improvement in image quality provides a more reliable visual foundation for expert analysis, thereby facilitating manual transcription, interpretation, and textual restoration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>This study provides a comprehensive review of recent advances in ancient script recognition, with a particular focus on the challenges posed by data scarcity, long-tail distribution, and noisy imagery. By categorizing scripts into phonographic and logographic types, we highlight key methodological distinctions, such as the central role of contextual modeling in phonographic systems and the reliance on structural decomposition in logographic ones. We introduce two major strategies for addressing low-resource scenarios: data augmentation and external knowledge integration, each offering distinct advantages and limitations. Furthermore, we emphasize the critical yet underexplored challenge of noisy image recognition, underscoring the need for more semantically-aware denoising approaches. This work lays the groundwork for more unified and adaptive systems in the field of digital paleography and cultural heritage preservation.</p><p>Based on our synthesis and discussion of existing methods for ancient script recognition and processing, we suggest the following topics for future research in this field: This appendix details the collaborative methodology adopted to gather, screen, and classify the literature used throughout this study. Our aim was to construct a comprehensive and high-quality corpus of research on ancient script recognition.</p><p>The process was carried out by a team of eight members and spanned several weeks of iterative refinement.</p><p>Data sources and retrieval scope. We primarily relied on major academic databases, Scopus, Web of Science, IEEE Xplore, ACM Digital Library, and Google Scholar, due to their broad coverage, domain relevance, and reliable citation indexing. The search scope was limited to publications between 2000 and 2025 and restricted to English-language sources to ensure consistency in downstream processing.</p><p>Search strategy. The search queries were formulated using domain-specific terminology to retrieve a wide range of relevant literature. Core keywords included "ancient script", "historical manuscript", "character recognition", "OCR", "ancient Greek inscriptions", "deep learning", and their combinations. To maximize coverage, we iteratively refined our search terms and balanced precision and recall through repeated search runs.</p><p>Screening and Quality Control. An initial retrieval yielded over 1,000 records. Duplicate entries were identified using title matching. We then performed title and abstract screening to ensure relevance, based on the following criteria: (1)</p><p>The paper must explicitly address the recognition of ancient or historical scripts; (2) It must propose a technical method, provide data, or report empirical findings; (3) Purely historical or cultural studies without computational contributions were excluded. For ambiguous cases, the full-text review was conducted to determine eligibility.</p><p>Linguistic and Typological Taxonomy. Under the guidance of linguists and archaeologists, we developed a hierarchical taxonomy for ancient scripts. First, we divided the scripts into two major categories: logographic and phonographic, and initially identified over 30 candidate script classes. Based on the literature search and review outcomes, we narrowed the scope to 17 representative script types. Rare script categories with insufficient data or lacking empirical contributions were excluded from further analysis. The 17 core script types were then refined into our hierarchical taxonomy according to their geographic origin and visual characteristics, as these factors typically influence recognition strategies.</p><p>Indexing Workflow. After processing, 192 papers were compiled into a structured database. Each paper was annotated with the following metadata: (i) script category (e.g., Latin, Maya, Egyptian); (ii) methodological approach (e.g., classic machine learning, deep learning, human-in-the-loop); (iii) task type (OCR, segmentation, writer identification); (iv) challenges in ancient script recognition (data scarcity, degradation); (v) main datasets used; and (vi) publication year.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of different ancient scripts as phonographic scripts and logographic scripts.</figDesc><graphic coords="2,120.87,95.04,406.98,137.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Annual cumulative counts of published papers and granted patents for ancient script recognition using deep learning from 2000 to 2024.</figDesc><graphic coords="4,217.26,95.04,214.20,162.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Representative example of the imbalanced distribution in ancient script datasets.</figDesc><graphic coords="5,116.28,95.04,342.72,138.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Representative examples of the different visual degradation on ancient scripts. (a) Greek inscriptions with cracks and damaged edges, (b) Egyptian hieroglyphs with scratches, (c) Sumerian cuneiform with erosion, (d)-(e) OBIs with erosion and synthetic noises, respectively.</figDesc><graphic coords="6,142.29,95.04,364.15,92.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 (</head><label>4</label><figDesc>a)-(d) shows examples of noisy ancient images, including Greek inscriptions, Egyptian hieroglyphs, Sumerian cuneiform, and OBIs, highlighting the prevalence of mixed noise types. Fig. 4 (e) contrasts an OBI image overlaid with synthetic Gaussian noise, revealing significant visual differences and underscoring the challenge real-world noise poses, thereby reducing the effectiveness of models trained only on synthetic noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Geographic distribution of major ancient scripts. Logographic scripts are represented with colored circles, and phonographic scripts with colored crosses. Each script family shares a color hue, with script subtypes shown in varying shades. Marker positions reflect approximate historical usage regions.</figDesc><graphic coords="8,110.16,95.04,428.40,111.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Examples of logographic scripts from diverse ancient civilizations, including Maya hieroglyphs, Egyptian hieroglyphs, Oracle Bone inscriptions, Bronze inscriptions, Chu bamboo slips, Small Seal script, Mesopotamian cuneiform, and Proto-Elamite script. These scripts were inscribed on various writing media, e.g., stone, bone, bronze, bamboo, and clay. In the Ancient Egyptian panel, red boxes highlight royal names enclosed in cartouches, while orange circles indicate characters derived from real-world objects and animals.</figDesc><graphic coords="9,73.44,95.04,428.38,229.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>leveraged the Detectron2 framework to propose a deep learning-based instance-level segmentation method. As one of the first systematic studies focused on symbol-level instance recognition, their model, based on a fine-tuned Mask R-CNN, achieved robust segmentation performance across heterogeneous image sources, providing a precise foundation for transcription and stylistic analysis under complex visual conditions [66]. Concurrently, scholars have expanded their focus to the semantic interpretation of hieroglyphic images, aiming to extract the underlying meanings encoded within the script. Elnabawy et al. proposed a comprehensive recognition pipeline based on image processing and optical character recognition (OCR), encompassing image acquisition, automatic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>3. 3 . 1</head><label>31</label><figDesc>Oracle Bone Inscriptions. Oracle Bone Inscription (OBI) is the earliest known form of Chinese writing, primarily used during the Shang dynasty (around 1,300 BCE) for divination and ritual inscriptions. OBIs were typically carved on turtle plastrons and ox scapulae, as shown in Fig. 6. Due to centuries of weathering, fragmentation, and erosion, automatic recognition of OBIs presents unique technical challenges. Early studies primarily relied on image processing techniques and handcrafted feature extraction for character enhancement and recognition. Meng et al. [109] employed a combination of Hough Transform, Gaussian filtering, and image binarization to effectively separate characters from background noise, reducing false positives and improving image clarity and subsequent recognition accuracy. Sun et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>3. 3 . 2</head><label>32</label><figDesc>Bronze Inscriptions. Bronze inscriptions, also known as Jinwen, represent a significant form of ancient Chinese writing. They were widely used from the Western Zhou period (around 700 BCE) through to the Spring and Autumn and Warring States periods, commonly cast or engraved on ritual bronzeware such as ceremonial vessels and bells, as shown in Fig.6. Compared to oracle bone script, bronze inscriptions feature more rounded and robust strokes with a greater diversity in structure. However, due to centuries of burial, the surfaces of these artifacts often exhibit oxidation, corrosion, and wear, which pose substantial challenges for image recognition and feature extraction. To address the recognition of bronze inscriptions, Wu et al.<ref type="bibr" target="#b177">[177]</ref> proposed a recognition method based on a backpropagation neural network tailored for bronze image classification. Their study constructed a training dataset comprising 100 distinct bronze images, each associated with 10 samples, and an independent test set containing 256 samples. The model relied on a feature extraction strategy that fused pixel-level information with local features, ultimately achieving a recognition accuracy of 93.3%. This result demonstrates the potential of deep neural networks in handling visually complex bronze images with noisy backgrounds. Wang et al. [167] proposed a more advanced convolutional neural network (CNN)-based model for the recognition of bronze inscriptions. Their work systematically compared and optimized multiple CNN architectures to adapt to the unique morphological characteristics and surface variability of bronze inscriptions. The model design incorporated a spatial transformer network (STN) to enhance focus on key inscription regions and employed Implicit Semantic Data Augmentation to improve the model's generalizability and robustness. Evaluated on a large-scale bronze inscription dataset, the final model achieved an accuracy of 91.2%, validating the effectiveness and scalability of deep learning approaches in the interpretation of complex historical scripts.3.3.3 Chu Bamboo Slips.During the Warring States period, bamboo slips were widely used in the state of Chu (around 400 BCE) to document political, philosophical, and historical texts. As shown in Fig.6, these texts were inscribed on thin, narrow strips of bamboo using brush and ink, a lightweight and flexible medium that preceded the invention of paper. Chu slip characters feature fluid strokes and loose structures, making them more challenging to recognize than carved scripts. To better capture such stylistic complexity, Wu et al.<ref type="bibr" target="#b176">[176]</ref> proposed a transformer-based recognition model incorporating multi-scale attention and MLP modules, along with graphical augmentation to improve training stability and convergence. Recognition tasks involving Chu slips often encounter issues such as stroke overlap, uneven ink distribution, and background noise. Addressing character segmentation under these conditions, Cao et al.<ref type="bibr" target="#b22">[22]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>bias and uncovering latent structural patterns. Extending this line of work, Born et al.<ref type="bibr" target="#b19">[19]</ref> introduced a multimodal representation learning framework that integrates image encoders with language models. They also proposed a bootstrapped classification algorithm and sequence modeling strategies to advance structural understanding across multiple dimensions, including character clustering, composite grapheme construction, numeric symbol disambiguation, and document structure analysis. These studies indicate that while Proto-Elamite graphemes exhibit a high degree of graphical freedom, their internal organization is nonetheless systematic, providing a technical pathway for deciphering this yet-untranslated script. In the DeepScribe project, Williams et al.<ref type="bibr" target="#b175">[175]</ref> implemented a deep learning pipeline combining RetinaNet for symbol localization and ResNet for symbol classification. This work achieved high accuracy in both detection and classification tasks, demonstrating the feasibility of automated transcription for ancient cuneiform scripts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Complex ligature patterns are a defining feature across Sanskrit, Tamil, Old Latin, Cyrillic, and Classical Arabian scripts, and appear extensively in their manuscript traditions. These structures introduce shared challenges in OCR, often requiring similar recognition strategies across languages.</figDesc><graphic coords="15,73.44,95.04,428.36,125.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>4. 1</head><label>1</label><figDesc>Ancient Indian script 4.1.1 Sanskrit Script. Sanskrit is one of the oldest and most culturally authoritative languages of the Indian subcontinent. It was originally written in the Brahmi script (around 300 BCE) and later evolved into various writing systems, including Devanagari (around 700 CE). As shown in Fig. 7, due to its complex ligature structures, the Sanskrit script often exhibits non-linear visual configurations, leading to blurred character boundaries and increased recognition difficulty. To address these challenges, researchers have proposed various frameworks based on both traditional approaches and deep learning techniques for high-precision character-or word-level recognition. In earlier studies, Dwivedi et al. proposed a recognition framework that integrates edge detection, genetic algorithms, and SVM classifiers to tackle issues related to structural redundancy and segmentation ambiguity in handwritten Sanskrit scripts [46]. Dineshkumar et al. developed an offline OCR system based on a feedforward neural network that encompasses the full pipeline from image preprocessing and character segmentation to feature extraction and classification [42]. Shih et al. designed a two-stage classification system that employs k-nearest neighbors for coarse classification, followed by multi-class SVMs for refined recognition, achieving an accuracy of 85.6% across 65 classes of handwritten Sanskrit scripts [151]. The diverse font variants and highly compact writing style of classical Sanskrit scripts introduce additional challenges to the recognition task. With the advancement of deep learning, researchers have increasingly adopted architectures such as CNNs, LSTMs, and attention mechanisms to enhance context understanding and sequential modeling. Dwivedi et al. proposed an OCR system that combines a CNN-based feature extractor with an attention-driven Bidirectional LSTM (Bi-LSTM) sequence model, specifically addressing the challenge of recognizing long compound ligatures in classical Sanskrit texts, supported by a large-scale hybrid dataset consisting of both real and synthetic samples [45]. Kataria et al. introduced a CNN-Bi-LSTM-based model for printed Sanskrit text recognition, achieving 98.6% character-level and 93.5% paragraph-level accuracy without relying on any language model, thereby significantly improving the system's practical applicability [85]. Lomte et al., targeting handwritten Sanskrit, constructed a dataset of 70,000 images and proposed three variants of a four-fold CNN architecture, benchmarking against AlexNet [101]. Kulkarni et al. demonstrated through comparative experiments that CNN-based methods offer superior performance in text-background separation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Examples of diacritical marks from Ancient Greek, Cyrillic, and Hebrew manuscripts, including breathing signs, accentuation, cantillation marks, and vocalization dots, contribute to the visual and typographic complexity of the scripts. Representative examples of diacritical marks are highlighted with red lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>4. 2 . 3</head><label>23</label><figDesc>Cyrillic Scripts. The Cyrillic script is derived from the Greek alphabet and emerged around the 8th century CE. It serves as the primary writing system for the Slavic language family. Cyrillic scripts exhibit two notable features relevant to text recognition: the frequent use of ligatures in the manuscripts, as illustrated in Fig.7, and the presence of diverse diacritical marks, which are highlighted with red lines in Fig.8. To address the challenge of ligatures in handwritten Cyrillic texts, Cojocaru et al. proposed a staged OCR strategy specifically for Romanian Cyrillic printed documents from the 18th to 20th centuries. Their method combined original glyph recognition with a ligature substitution mechanism, significantly reducing recognition errors between documents of different historical periods and enabling standardized transcription and full text retrieval functionalities<ref type="bibr" target="#b30">[30]</ref>. In terms of resource development, Nurseitov et al.introduced the HKR dataset, the first large-scale corpus covering handwritten Kazakh and Russian texts, comprising over 63,000 sentences and 700,000 words<ref type="bibr" target="#b123">[123]</ref>. Coupled with automated annotation and segmentation workflows, HKR provides crucial support for HTR in low-resource Slavic languages and has been systematically evaluated across mainstream models. Cristea et al. developed a comprehensive automated workflow for recognizing and transcribing ancient Romanian Cyrillic manuscripts into modern Latin script, along with constructing the Romanian Old Cyrillic Corpus (ROCC) dataset<ref type="bibr" target="#b32">[32]</ref>. Their system integrated Faster R-CNN-based object detection, character recognition, and sequence modeling techniques, and further employed self-training and data augmentation to address noise, font heterogeneity, and linguistic variability present in historical manuscripts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Liebeskind et al. introduced a deep learning framework for classifying Hebrew texts by writing period, systematically comparing paragraph vectors, CNN, and RNN, and demonstrating the effectiveness of deep models in fine-grained period classification tasks<ref type="bibr" target="#b93">[93]</ref>. Tobing et al., using a dataset derived from the 1972 edition of the Book of Isaiah, conducted a systematic comparison of four CNN architectures and found that AlexNet and LeNet-5 performed best in ancient Hebrew character recognition, achieving over 94% accuracy<ref type="bibr" target="#b164">[164]</ref>. Shapira et al.<ref type="bibr" target="#b145">[145]</ref> present an interdisciplinary approach to clustering medieval Hebrew manuscripts based on script style. By combining paleographic expertise with computer vision and deep learning techniques, including SIFT, LBP, and ControlNet, the authors aim to automatically group manuscripts by visual features and handwriting style, thereby revealing new sub-clusters beyond traditional classifications.For the stylistic attribution and writer identification of historical documents, Likforman-Sulem et al. proposed a knowledge-based system integrating calligraphic rules and textual knowledge, which significantly reduced manuscript authentication time<ref type="bibr" target="#b94">[94]</ref>. Furthermore, Bar-Yosef et al.<ref type="bibr" target="#b10">[11]</ref> developed a comprehensive processing pipeline, including Diao et al. adaptive binarization, unsupervised character extraction based on morphological erosion, and local geometric featurebased writer classification, achieving great accuracy across 34 writer samples. Faigenbaum-Golovin et al. systematicallyreviewed the computational analysis workflow for ancient Hebrew inscriptions. They identified major challenges such as the lack of standardized benchmarks and sufficient annotated data, and proposed future research directions emphasizing self-supervised learning and multimodal data integration<ref type="bibr" target="#b53">[53]</ref>. To enhance the quality of OCR outputs, Suissa et al.<ref type="bibr" target="#b157">[157]</ref> introduced a multi-stage optimization approach that combines period-specific error injection with automated DNN architecture search, significantly reducing the CER and WER in historical Hebrew documents under low-resource conditions .4.4 Ancient Arabian Scripts4.4.1 Classical Arabian Scripts. Classical Arabic script, the direct ancestor of the modern Arabic writing system, emerged around 700 CE and was standardized during the early Islamic period. It became widely used across the Arabian Peninsula for religious, administrative, and literary purposes. Its writing characteristics include a high degree of ligature, cursive flow, and contextual variation in character shapes. As illustrated in Fig.7, Classical Arabic script demonstrates extensive ligature formations, which are integral to its calligraphic structure and pose challenges for segmentation in OCR tasks. To address the complexity of ligatures and font variability in printed Arabic documents, Qaroush et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>simulated real-world noise in manuscripts by constructing a large-scale dataset containing 30 million images and enhanced transcription quality through page and line segmentation techniques. Saeed et al. [142] proposed a historical handwritten Arabic manuscript dataset, Muharaf, and established a baseline system for handwritten text recognition (HTR) based on it. They introduced a three-stage CNN model, Start-Follow-Read, providing new benchmark resources and experimental results for historical handwritten Arabic text recognition tasks. To overcome the sequential modeling limitations of traditional RNNs, Momeni et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Several other works aimed to improve recognition performance by enhancing model robustness. Liu et al. grouped visually similar variants via spectral clustering and utilized label propagation for robust recognition, addressing OBI script variability and mitigating noise interference [96]. Guo et al. proposed an improved Inception-v3-based method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>( 1 )</head><label>1</label><figDesc>The development of ancient script recognition methods should prioritize feature-based rather than script-specific approaches, with the goal of establishing robust and generalizable frameworks that can handle a wide range of scripts; (2) Linguistic context and character components are critical elements in the recognition process. Effective recognition systems should integrate visual, structural, and linguistic cues to enhance performance, particularly for rare or undeciphered characters; (3) Future research should focus on modeling the historical evolution and stylistic variations of ancient scripts, addressing the associated feature shifting over time, and incorporating strategies to mitigate the impact of real-world degradation in script images. A Appendix: Literature Collection and Management Workflow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of prior surveys for ancient script recognition methods based on coverage of scripts, taxonomy, datasets provided, and technical challenges.</figDesc><table><row><cell>3]</cell><cell>2020</cell><cell></cell><cell>7 scripts without taxonomy</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>2025</cell><cell>✓</cell><cell>17 scripts with taxonomy</cell><cell>✓</cell><cell>✓</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Old Latin Scripts. Latin script was widely used throughout ancient Rome and later for the Roman languages and other successor cultures. The Latin script originated around 700 BCE and underwent centuries of evolution, resulting in a wide variety of writing styles that pose significant challenges for recognition. Examples of Old Latin Scripts can be found in Fig. 7. Researchers have proposed solutions in various aspects, including OCR model training, font classification, and corpus construction. Springmann et al. focused on the challenges of spelling variations and font differences in Latin printed documents, constructing morphological lexicons and dictionary-supported OCR systems.</figDesc><table><row><cell>by integrating curve fitting and Bonferroni correction methods, enabling high-precision scribe identification across</cell></row><row><cell>inscriptions from the Classical to Hellenistic periods [140].</cell></row><row><cell>Ancient Greek texts and earlier Aegean scripts pose considerable challenges due to their complex glyph contours,</cell></row><row><cell>local visual similarities, and severe manuscript degradation. To address these difficulties, Assael et al. introduced the</cell></row><row><cell>LSTM-based PYTHIA system, which achieves a superior restoration performance of damaged text compared to human</cell></row><row><cell>epigraphers for the first time [8]. For undeciphered scripts, Corazza et al. emphasized the necessity of unsupervised</cell></row><row><cell>approaches and proposed a validation framework utilizing already deciphered systems to guide the application of</cell></row><row><cell>machine learning techniques to undeciphered scripts [31].</cell></row><row><cell>4.2.2</cell></row><row><cell>. Building upon</cell></row><row><cell>this, Papaodysseus et al. employed multiple statistical tests and contour matching techniques, achieving promising</cell></row><row><cell>classification performance among ancient Athenian inscriptions [126]. Rousopoulos et al. further refined the approach</cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p>By combining training on Fell fonts with post-processing, they improved recognition rates to over 90%</p><ref type="bibr" target="#b154">[154]</ref></p>.</p>Reul et al.    </p>proposed a mixed OCR model based on the Calamari framework, trained on multilingual data, demonstrating strong cross-domain transferability</p><ref type="bibr" target="#b134">[134]</ref></p>. Additionally, Brodic et al. addressed the classification of Latin and Fraktur fonts in German historical documents by leveraging image texture analysis and gray-level co-occurrence matrix features, coupled with a clustering classifier, achieving promising performance in glyph style classification tasks</p><ref type="bibr" target="#b20">[20]</ref></p>.</p>At the data level, historical Latin documents are highly heterogeneous, while also exhibiting rich spelling variations and limited availability of high-quality annotated resources. These factors greatly complicate the construction of OCR and HTR systems. To better support historical Latin manuscript recognition tasks, standardized evaluation resources and multitask datasets have been progressively developed. Cloppet et al. organized the ICFHR 2016 Competition on Medieval Handwriting, establishing two subtasks: (i) fine-grained font classification, and (ii) blurred mixed font classification, based on a standard dataset comprising twelve Latin script categories sourced from multiple European library collections. Various classification models, including VGG networks, ResNets, and i-vector frameworks, were employed to assess system performance in understanding font evolution</p><ref type="bibr" target="#b29">[29]</ref></p>. Furthermore, Clérice et al. introduced the CATMuS Medieval dataset, covering documents from the 8th to 16th centuries across ten languages and over 200 manuscripts, with unified transcription standards and annotations for language, genre, century, and font metadata, providing a robust benchmark for HTR model training and cross-linguistic, cross-genre evaluation</p><ref type="bibr" target="#b28">[28]</ref></p>. In terms of modeling approaches, Moudgil et al. applied a hybrid CNN-LSTM strategy for medieval manuscript recognition, where CNNs were responsible for image feature extraction and LSTMs preserved sequential character dependencies, achieving better word-level recognition performance</p><ref type="bibr" target="#b114">[114]</ref></p>. Jindal et al. further enhanced recognition performance to 96.97% by employing a CNN-BiLSTM architecture combined with Bayesian optimization, demonstrating the strong robustness of deep hybrid architectures for complex fonts and stylized character forms</p><ref type="bibr" target="#b79">[79]</ref></p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Tesseract engine. Their hierarchical training approach achieved a CER of 13%, surpassing baseline systems by more than 10%. As research progressed, Malhotra et al. proposed an end-to-end recognition model exploiting Bi-LSTM, attention mechanisms, and CNNs for word-level recognition [105]. The model significantly improved generalization and robustness on the HHD-Ethiopic dataset, achieving CERs of 17.95% and 29.95% on different evaluation sets. Beyond text characters, Ali et al. compiled a large-scale dataset of 51,952 handwritten Ge'ez numeral images [4], and through the evaluation of six CNN architectures, the best-performing model achieved an accuracy of 96.21%, further demonstrating the effectiveness of deep learning in handling complex handwritten scripts. In early Ge'ez handwritten text recognition, Getu et al. [60] developed an OCR system based on Deep Belief Networks, achieving a 93.8% accuracy rate in the classification of 24 basic characters extracted and annotated from 200 manuscript pages. Based on this, Demilew et al.[35, 36] introduced a CNN-based recognition system, achieving promising performance through systematic preprocessing and training on a dataset of 22,913 character images. In the domain of scene text recognition, Addis et al.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>al. introduced a hybrid system that standardizes background textures, extracts structural skeleton features, and captures global relationships, showing strong results on both single-scene and multi-scene datasets<ref type="bibr" target="#b59">[59]</ref>. Their method normalizes images from different settings into a unified space to facilitate efficient structural feature extraction. Zhang et al. leveraged a self-supervised network that incorporates shape similarity constraints to rejoin fragmented ancient scripts, thereby boosting the ability of deep learning models to handle incomplete glyphs in specialized scenarios<ref type="bibr" target="#b189">[189]</ref>. Further addressing domain adaptation, Wang et al. proposed an unsupervised discriminative consistency network, which relies on consistent pseudo-labeling across different augmentations to improve robustness in real-world oracle bone scenarios; through unsupervised transformation loss, it learns more discriminative features and remains robust against wear, stains, and distortion<ref type="bibr" target="#b171">[171]</ref>. Domain-aware feature extraction also plays a vital role. Gao et al.<ref type="bibr" target="#b58">[58]</ref> embed specialized field knowledge into a set of OBI radical prototypes, facilitating more effective structural understanding and retrieval even amid noise or physical damage. Similarly, Tang et al.<ref type="bibr" target="#b163">[163]</ref> explore alignment, denoising, and script segmentation in large-scale ancient manuscript collections, devising a lightweight attention-based model, ShuiNet-A, to tackle data imbalance and noise-related difficulties.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>develops a denoising framework based on script writing norms, adding four local branches to reduce stroke adhesion caused by missing details. Researchers have also turned to transformer-based architectures; for example, CharFormer<ref type="bibr" target="#b147">[147]</ref> injects glyph-specific information into the backbone network through self-attention, striving to enhance ancient script noise removal without mistakenly removing genuine strokes.GANs are also widely applied to enhance ancient script images. Xing<ref type="bibr" target="#b179">[179]</ref> underscores a data-centric view, synthesizing various noise types to augment training data while using Focal Loss to prioritize difficult samples. Wang et al.<ref type="bibr" target="#b173">[173]</ref> further explore GAN-based augmentation, devising a loop network and denoising autoencoder for Chinese calligraphy images that generate noise attention maps to guide artifact removal. Shi et al.<ref type="bibr" target="#b148">[148]</ref> propose RCRN, a real-world script restoration network integrating skeleton extraction and multi-scale features, demonstrating robust performance on a</figDesc><table /><note><p><p><p><p><p>new dataset of 1,606 degraded scripts. Wenjun et al.</p><ref type="bibr" target="#b174">[174]</ref> </p>introduce EA-GAN, a dual-branch approach incorporating example-based guidance to reconstruct severely damaged ancient Chinese scripts, reporting marked improvements in PSNR and SSIM. More recently, Gao</p><ref type="bibr" target="#b59">[59]</ref> </p>employs random transformations and Gaussian noise to simulate realistic corrosion and introduces a Reconstruction Bias Coefficient in the loss function to balance these influences.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Techniques that leverage radical decomposition, visual templates, or knowledge graphs have proven particularly effective for low-frequency classes, especially in boosting tail-class accuracy.By contrast, phonographic scripts tend to have a much smaller character inventory, usually under a few hundred units, and thus achieve near-saturation in single-character recognition under sufficient training. The primary difficulty instead arises from handwritten forms, where stylistic variability, frequent ligatures, and diacritical marks introduce high intra-class variation. These phenomena often break the assumption of character segmentation, rendering geometric data augmentation or simple convolutional architectures ineffective. Consequently, robust phonographic OCR requires methods that can generalize across handwriting styles, disambiguate overlapping glyphs, and integrate visual layout with linguistic structure.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>To preserve fragile ancient artifacts and facilitate purposes such as readability and data processing, ancient scripts are often transformed into images via digitization methods.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The reference data are sourced from Google Scholar, Scopus, and The Lens.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>These three characters mean timber, forest, and wood, respectively.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Dataset available at: https://huggingface.co/datasets/HamdiJr/Egyptian_hieroglyphs</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No.<rs type="grantNumber">62476111</rs>), the <rs type="funder">Department of Science and Technology of Jilin Province, China</rs> (<rs type="grantNumber">20230201086GX</rs>), the <rs type="funder">"Paleography and Chinese Civilization Inheritance and Development Program" Collaborative Innovation Platform</rs> (No.<rs type="grantNumber">G3829</rs>), the <rs type="funder">National Social Science Foundation of China</rs> (No. <rs type="grantNumber">23VRC033</rs>), and the interdisciplinary cultivation project for young teachers and students at <rs type="funder">Jilin University, China</rs> (No. <rs type="grantNumber">2024-JCXK-04</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jkpcvuN">
					<idno type="grant-number">62476111</idno>
				</org>
				<org type="funding" xml:id="_wxYfYTN">
					<idno type="grant-number">20230201086GX</idno>
				</org>
				<org type="funding" xml:id="_KnjeNnE">
					<idno type="grant-number">G3829</idno>
				</org>
				<org type="funding" xml:id="_zXpCYGR">
					<idno type="grant-number">23VRC033</idno>
				</org>
				<org type="funding" xml:id="_EU2PH95">
					<idno type="grant-number">2024-JCXK-04</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Printed ethiopic script recognition by using lstm networks</title>
		<author>
			<persName><forename type="first">Direselign</forename><surname>Addis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan-Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van-Dai</forename><surname>Ta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on System Science and Engineering (ICSSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ethiopic natural scene text recognition using deep learning approaches</title>
		<author>
			<persName><forename type="first">Direselign</forename><surname>Addis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan-Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van-Dai</forename><surname>Ta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances of Science and Technology: 7th EAI International Conference</title>
		<meeting><address><addrLine>Bahir Dar, Ethiopia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-08-02">2020. 2019. August 2-4, 2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
	<note>ICAST</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Handwritten recognition: A survey</title>
		<author>
			<persName><forename type="first">May</forename><surname>Mowaffaq Al-Taee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonia</forename><surname>Ben Hassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mondher</forename><surname>Neji</surname></persName>
		</author>
		<author>
			<persName><surname>Frikha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 4th International Conference on Image Processing, Applications and Systems (IPAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="199" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Handwritten Geez Digit Recognition Using Deep Learning</title>
		<author>
			<persName><forename type="first">Mukerem</forename><surname>Ali Nur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mesfin</forename><surname>Abebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Sharma Rajendran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Computational Intelligence and Soft Computing</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page">8515810</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prototype Calibration with Synthesized Samples for Zero-Shot Chinese Character Recognition</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu-Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp48485.2024.10446790</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024-04-14">2024</date>
			<biblScope unit="page" from="6295" to="6299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ethiopic Character Recognition Using Direction Field Tensor</title>
		<author>
			<persName><forename type="first">Yaregal</forename><surname>Assabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Bigun</surname></persName>
		</author>
		<idno type="DOI">10.1109/icpr.2006.507</idno>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition (ICPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="284" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structural and Syntactic Techniques for Recognition of Ethiopic Characters</title>
		<author>
			<persName><forename type="first">Yaregal</forename><surname>Assabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Bigun</surname></persName>
		</author>
		<idno type="DOI">10.1007/11815921_12</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2006-08-17">2006. August 17-19, 2006</date>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Restoring ancient text using deep learning: a case study on Greek epigraphy</title>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thea</forename><surname>Sommerschield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Prag</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1668</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6368" to="6375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Restoring and attributing ancient texts using deep neural networks</title>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Assael</surname></persName>
			<idno type="ORCID">0000-0001-7408-3847</idno>
		</author>
		<author>
			<persName><forename type="first">Thea</forename><surname>Sommerschield</surname></persName>
			<idno type="ORCID">0000-0002-6965-8105</idno>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahyar</forename><surname>Bordbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
			<idno type="ORCID">0000-0001-9188-7425</idno>
		</author>
		<author>
			<persName><forename type="first">Marita</forename><surname>Chatzipanagiotou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Prag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-022-04448-z</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<title level="j" type="abbrev">Nature</title>
		<idno type="ISSN">0028-0836</idno>
		<idno type="ISSNe">1476-4687</idno>
		<imprint>
			<biblScope unit="volume">603</biblScope>
			<biblScope unit="issue">7900</biblScope>
			<biblScope unit="page" from="280" to="283" />
			<date type="published" when="2022-03-09">2022. 2022</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Handwritten Tamil character recognition using artificial neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Banumathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Nasira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Process Automation, Control and Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Binarization, character extraction, and writer identification of historical Hebrew calligraphy documents</title>
		<author>
			<persName><forename type="first">Itay</forename><surname>Bar-Yosef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Beckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klara</forename><surname>Kedem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itshak</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="89" to="99" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ancient Egyptian Hieroglyphs Segmentation and Classification with Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Barucci</surname></persName>
			<idno type="ORCID">0000-0002-3759-7512</idno>
		</author>
		<author>
			<persName><forename type="first">Chiara</forename><surname>Canfailla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costanza</forename><surname>Cucci</surname></persName>
			<idno type="ORCID">0000-0001-8534-7465</idno>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Forasassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Franci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Guarducci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Guidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Loschiavo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Picollo</surname></persName>
			<idno type="ORCID">0000-0003-1012-6048</idno>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Pini</surname></persName>
			<idno type="ORCID">0000-0001-8588-9897</idno>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Python</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Valentini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Argenti</surname></persName>
			<idno type="ORCID">0000-0001-7776-4015</idno>
		</author>
		<idno type="DOI">10.1007/978-3-031-20302-2_10</idno>
	</analytic>
	<monogr>
		<title level="m">Communications in Computer and Information Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="126" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Deep Learning Approach to Ancient Egyptian Hieroglyphs Classification</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Barucci</surname></persName>
			<idno type="ORCID">0000-0002-3759-7512</idno>
		</author>
		<author>
			<persName><forename type="first">Costanza</forename><surname>Cucci</surname></persName>
			<idno type="ORCID">0000-0001-8534-7465</idno>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Franci</surname></persName>
			<idno type="ORCID">0000-0003-1472-8409</idno>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Loschiavo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Argenti</surname></persName>
			<idno type="ORCID">0000-0001-7776-4015</idno>
		</author>
		<idno type="DOI">10.1109/access.2021.3110082</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<title level="j" type="abbrev">IEEE Access</title>
		<idno type="ISSNe">2169-3536</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="123438" to="123447" />
			<date type="published" when="2021">2021. 2021</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pho (SC)-CTC-a hybrid approach towards zero-shot word image recognition</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanda</forename><surname>Sukalpa</surname></persName>
		</author>
		<author>
			<persName><surname>Narayanan C Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="51" to="63" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Character retrieval of vectorized cuneiform script</title>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Bogacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Mara</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdar.2015.7333777</idno>
	</analytic>
	<monogr>
		<title level="m">2015 13th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-08">2015</date>
			<biblScope unit="page" from="326" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Digital assyriology-advances in visual cuneiform analysis</title>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Bogacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Mara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Computing and Cultural Heritage (JOCCH)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mara</forename><forename type="middle">H</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Subglyphs Bogacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feldmann</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Introduction</title>
		<idno type="DOI">10.7560/750661-003</idno>
	</analytic>
	<monogr>
		<title level="m">Maya Glyphs</title>
		<imprint>
			<publisher>University of Texas Press</publisher>
			<date type="published" when="2018-11-12">2018 Nov 12</date>
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Compositionality of Complex Graphemes in the Undeciphered Proto-Elamite Script using Image and Text Embedding Models</title>
		<author>
			<persName><forename type="first">Logan</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Willis</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.362</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4136" to="4146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">proto-Elamite, adj. &amp; n.</title>
		<idno type="DOI">10.1093/oed/3326976168</idno>
	</analytic>
	<monogr>
		<title level="m">Oxford English Dictionary</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2023-03-02">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Identification of fraktur and latin scripts in german historical documents using image texture analysis</title>
		<author>
			<persName><forename type="first">Darko</forename><surname>Brodić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessia</forename><surname>Amelio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoran</forename><forename type="middle">N</forename><surname>Milivojević</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="379" to="395" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How to Tell Ancient Signs Apart? Recognizing and Visualizing Maya Glyphs with CNNs</title>
		<author>
			<persName><forename type="first">Gülcan</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<idno type="DOI">10.1145/3230670</idno>
	</analytic>
	<monogr>
		<title level="j">Journal on Computing and Cultural Heritage</title>
		<title level="j" type="abbrev">J. Comput. Cult. Herit.</title>
		<idno type="ISSN">1556-4673</idno>
		<idno type="ISSNe">1556-4711</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2018-12-05">2018. 2018</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Character segmentation and restoration of Qin-Han bamboo slips using local auto-focus thresholding method</title>
		<author>
			<persName><forename type="first">Songxiao</forename><surname>Cao</surname></persName>
			<idno type="ORCID">0000-0001-5122-3472</idno>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dailiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-022-11988-z</idno>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<title level="j" type="abbrev">Multimed Tools Appl</title>
		<idno type="ISSN">1380-7501</idno>
		<idno type="ISSNe">1573-7721</idno>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="8199" to="8213" />
			<date type="published" when="2022-02-01">2022. 2022</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zero-shot learning based approach for medieval word recognition using deep-learned features</title>
		<author>
			<persName><forename type="first">Sukalpa</forename><surname>Chanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jochem</forename><surname>Baas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Haitink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Hamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Stutzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lambert</forename><surname>Schomaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="345" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognizing Bengali Word Images - A Zero-Shot Learning Perspective</title>
		<author>
			<persName><forename type="first">Sukalpa</forename><surname>Chanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Haitink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><forename type="middle">Kumar</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jochem</forename><surname>Baas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umapada</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lambert</forename><surname>Schomaker</surname></persName>
		</author>
		<idno type="DOI">10.1109/icpr48806.2021.9412607</idno>
		<ptr target="https://doi.org/10.1109/ICPR48806.2021.9412607" />
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-01-10">2021</date>
			<biblScope unit="page" from="5603" to="5610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A study on encoding-based oracle bone script recognition</title>
		<author>
			<persName><forename type="first">Tingzhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoyao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyu</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoteng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Yueh</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Writing Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="281" to="290" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stroke-based autoencoders: Self-supervised learners for efficient zero-shot Chinese character recognition</title>
		<author>
			<persName><forename type="first">Zongze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">1750</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Survey of Optical Character Recognition Techniques on Indic Script</title>
		<author>
			<persName><forename type="first">Ramya</forename><surname>Chirimilla</surname></persName>
		</author>
		<idno type="DOI">10.1149/10701.6507ecst</idno>
	</analytic>
	<monogr>
		<title level="j">ECS Transactions</title>
		<title level="j" type="abbrev">ECS Trans.</title>
		<idno type="ISSN">1938-5862</idno>
		<idno type="ISSNe">1938-6737</idno>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6507" to="6514" />
			<date type="published" when="2022-04-24">2022. 2022</date>
			<publisher>The Electrochemical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CATMuS Medieval: A multilingual large-scale cross-century dataset in Latin script for handwritten text recognition and beyond</title>
		<author>
			<persName><forename type="first">Ariane</forename><surname>Thibault Clérice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malamatenia</forename><surname>Pinche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alix</forename><surname>Vlachou-Efstathiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Chagué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Gille Levenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Brisville-Fertin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><surname>Boschetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><surname>Gervers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="174" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ICFHR2016 Competition on the classification of medieval handwritings in latin script</title>
		<author>
			<persName><forename type="first">Florence</forename><surname>Cloppet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Véronique</forename><surname>Eglin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Stutzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="590" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optical Character Recognition Applied to Romanian Printed Texts of the 18th to 20th Century</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Colesnicov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludmila</forename><surname>Malahov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tudor</forename><surname>Bumbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Journal of Moldova</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="106" to="117" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Computational methods for undeciphered scripts</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Corazza</surname></persName>
		</author>
		<idno type="DOI">10.30682/9791254774038</idno>
	</analytic>
	<monogr>
		<title level="j">Lingue e linguaggio</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="311" to="331" />
			<date type="published" when="2022">2022. 2022</date>
			<publisher>Fondazione Bologna University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">From scan to text. Methodology, solutions and perspectives of deciphering old cyrillic Romanian documents into the Latin script. Knowledge, Language, Models</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Cristea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Pădurariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petru</forename><surname>Rebeja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Onofrei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="38" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The proto-Elamite writing system</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Elamite World. Routledge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="383" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The world&apos;s writing systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName><surname>Bright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ancient Geez Script Recognition Using Deep Convolutional Neural Network</title>
		<author>
			<persName><surname>Fitehalew Ashagrie Demilew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
		<respStmt>
			<orgName>Near East University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ancient Geez script recognition using deep learning</title>
		<author>
			<persName><forename type="first">Ashagrie</forename><surname>Fitehalew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boran</forename><surname>Demilew</surname></persName>
		</author>
		<author>
			<persName><surname>Sekeroglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SN Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1315</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Taye Girma Debelee, and Worku Gachena. 2023. Typewritten OCR Model for Ethiopic Characters</title>
		<author>
			<persName><forename type="first">Bereket</forename><surname>Siraw Deneke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosa</forename><surname>Tsegaye Aga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mesay</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abel</forename><surname>Mulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashenafi</forename><surname>Mulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abel</forename><surname>Abebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahel</forename><surname>Mekonnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiwot</forename><surname>Mulugeta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pan African Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="250" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Toward Zero-shot Character Recognition: A Gold Standard Dataset with Radical-level Annotations</title>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Diao</surname></persName>
			<idno type="ORCID">0000-0002-3269-8103</idno>
		</author>
		<author>
			<persName><forename type="first">Daqian</forename><surname>Shi</surname></persName>
			<idno type="ORCID">0000-0003-2183-1957</idno>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
			<idno type="ORCID">0000-0002-0065-4333</idno>
		</author>
		<author>
			<persName><forename type="first">Lida</forename><surname>Shi</surname></persName>
			<idno type="ORCID">0000-0001-5011-6931</idno>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Yue</surname></persName>
			<idno type="ORCID">0009-0008-9982-2026</idno>
		</author>
		<author>
			<persName><forename type="first">Ruihua</forename><surname>Qi</surname></persName>
			<idno type="ORCID">0000-0002-0554-6040</idno>
		</author>
		<author>
			<persName><forename type="first">Chuntao</forename><surname>Li</surname></persName>
			<idno type="ORCID">0000-0001-9836-1493</idno>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xu</surname></persName>
			<idno type="ORCID">0000-0001-8474-0767</idno>
		</author>
		<idno type="DOI">10.1145/3581783.3612201</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023-10-26">2023</date>
			<biblScope unit="page" from="6869" to="6877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">RZCR: Zero-shot Character Recognition via Radical-based Reasoning</title>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2023/73</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence Organization</publisher>
			<date type="published" when="2023-08">2023</date>
			<biblScope unit="page" from="654" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Oracle Bone Inscription Image Restoration via Glyph Extraction</title>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuntao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Heritage Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2025">2025. 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sanskrit Character Recognition System Using Neural Network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dineshkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suganthi</surname></persName>
		</author>
		<idno type="DOI">10.17485/ijst/2015/v8i1/52878</idno>
	</analytic>
	<monogr>
		<title level="j">Indian Journal of Science and Technology</title>
		<idno type="ISSN">0974-6846</idno>
		<idno type="ISSNe">0974-5645</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="2013">2013. 2013</date>
			<publisher>Indian Society for Education and Environment</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sanskrit Character Recognition System Using Neural Network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dineshkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suganthi</surname></persName>
		</author>
		<idno type="DOI">10.17485/ijst/2015/v8i1/52878</idno>
	</analytic>
	<monogr>
		<title level="j">Indian Journal of Science and Technology</title>
		<idno type="ISSN">0974-6846</idno>
		<idno type="ISSNe">0974-5645</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="2015-01-16">2015. 2015</date>
			<publisher>Indian Society for Education and Environment</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A Survey on Arabic Handwritten Script Recognition Systems</title>
		<author>
			<persName><forename type="first">Soumia</forename><surname>Djaghbellou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abderraouf</forename><surname>Bouziane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelouahab</forename><surname>Attia</surname></persName>
			<idno type="ORCID">0000-0003-1558-7273</idno>
		</author>
		<author>
			<persName><forename type="first">Zahid</forename><surname>Akhtar</surname></persName>
		</author>
		<idno type="DOI">10.4018/ijaiml.20210701.oa9</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Artificial Intelligence and Machine Learning</title>
		<idno type="ISSN">2642-1577</idno>
		<idno type="ISSNe">2642-1585</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2021-09-29">2021. 2021</date>
			<publisher>IGI Global</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deciphering Egyptian Hieroglyphs: Towards a New Strategy for Navigation in Museums</title>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Duque-Domingo</surname></persName>
			<idno type="ORCID">0000-0001-6649-5550</idno>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Javier</forename><surname>Herrera</surname></persName>
			<idno type="ORCID">0000-0001-8679-6617</idno>
		</author>
		<author>
			<persName><forename type="first">Enrique</forename><surname>Valero</surname></persName>
			<idno type="ORCID">0000-0002-0016-4473</idno>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Cerrada</surname></persName>
			<idno type="ORCID">0000-0002-8591-6581</idno>
		</author>
		<idno type="DOI">10.3390/s17030589</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<title level="j" type="abbrev">Sensors</title>
		<idno type="ISSNe">1424-8220</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">589</biblScope>
			<date type="published" when="2017-03-14">2017. 2017</date>
			<publisher>MDPI AG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An OCR for classical Indic documents containing arbitrarily long words</title>
		<author>
			<persName><forename type="first">Agam</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Saluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarvadevabhatla</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="560" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sanskrit word recognition using Prewitt&apos;s operator and support vector classification</title>
		<author>
			<persName><forename type="first">Namita</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neelam</forename><surname>Arya</surname></persName>
		</author>
		<idno type="DOI">10.1109/ice-ccn.2013.6528506</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference ON Emerging Trends in Computing, Communication and Nanotechnology (ICECCN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-03">2013</date>
			<biblScope unit="page" from="265" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A Novel Siamese Network for Few/Zero-Shot Handwritten Character Recognition Tasks</title>
		<author>
			<persName><forename type="first">Nagwa</forename><surname>Elaraby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherif</forename><surname>Barakat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amira</forename><surname>Rezk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers, Materials &amp; Continua</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The effects of orthographic depth on learning to read alphabetic, syllabic, and logographic scripts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miwa</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenc</forename><surname>Stavropoulou</surname></persName>
		</author>
		<author>
			<persName><surname>Hoxhallari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicoletta</forename><surname>Van Daal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria-Louisa</forename><surname>Polyzoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalis</forename><surname>Tsipa</surname></persName>
		</author>
		<author>
			<persName><surname>Petalas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reading research quarterly</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="438" to="468" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image Based Hieroglyphic Character Recognition</title>
		<author>
			<persName><forename type="first">Reham</forename><surname>Elnabawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rimon</forename><surname>Elias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Salem</surname></persName>
		</author>
		<idno type="DOI">10.1109/sitis.2018.00016</idno>
	</analytic>
	<monogr>
		<title level="m">2018 14th International Conference on Signal-Image Technology &amp; Internet-Based Systems (SITIS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-11">2018</date>
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unlocking Ancient Secrets: A Deep Learning Approach to Cuneiform Symbols Recognition</title>
		<author>
			<persName><forename type="first">Shahad</forename><surname>Elshehaby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mina</forename><surname>Al-Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alavikunhu</forename><surname>Panthakkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hussain</forename><forename type="middle">Al</forename><surname>Ahmad</surname></persName>
		</author>
		<idno type="DOI">10.1109/aset60340.2024.10708771</idno>
	</analytic>
	<monogr>
		<title level="m">2024 Advances in Science and Engineering Technology International Conferences (ASET)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024-06-03">2024</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">The state of decipherment of Proto-Elamite</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Englund</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">RECOGNIZING TAMIL PALM-LEAF MANUSCRIPT CHARACTERS USING HYBRIDIZED HUMAN PERCEPTION BASED FEATURES</title>
		<author>
			<persName><forename type="first">Paramasivam</forename><forename type="middle">Muthan</forename><surname>Eswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Manib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabeenian</forename><forename type="middle">Royappan</forename><surname>Savarimuthu</surname></persName>
		</author>
		<idno type="DOI">10.21917/ijivp.2021.0346</idno>
	</analytic>
	<monogr>
		<title level="j">ICTACT Journal on Image and Video Processing</title>
		<title level="j" type="abbrev">IJIVP</title>
		<idno type="ISSN">0976-9099</idno>
		<idno type="ISSNe">0976-9102</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2432" to="2440" />
			<date type="published" when="2021-05-01">2021. 2021</date>
			<publisher>ICT Academy</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Computational Handwriting Analysis of Ancient Hebrew Inscriptions – A Survey</title>
		<author>
			<persName><forename type="first">Shira</forename><surname>Faigenbaum-Golovin</surname></persName>
			<idno type="ORCID">0000-0003-0320-9726</idno>
		</author>
		<author>
			<persName><forename type="first">Arie</forename><surname>Shaus</surname></persName>
			<idno type="ORCID">0000-0003-3727-2774</idno>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Sober</surname></persName>
			<idno type="ORCID">0000-0001-5090-5551</idno>
		</author>
		<idno type="DOI">10.1109/mbits.2022.3197559</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE BITS the Information Theory Magazine</title>
		<title level="j" type="abbrev">IEEE BITS Inform. Theory Mag.</title>
		<idno type="ISSN">2692-4080</idno>
		<idno type="ISSNe">2692-4110</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022. 2022</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Advanced techniques for the decipherment of ancient scripts</title>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Ferrara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Tamburini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingue e linguaggio</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="239" to="259" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Segmentation of Maya Hieroglyphs through Fine-Tuned Foundation Models</title>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Fnu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Leight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Kate</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelsey</forename><surname>Clodfelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Thrasher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chowdhury</forename><surname>Mohammad Abid Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yenumula</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashnna</forename><surname>Gyawali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="540" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Automatic Egyptian hieroglyph recognition by retrieving images as texts</title>
		<author>
			<persName><forename type="first">Morris</forename><surname>Franken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<idno type="DOI">10.1145/2502081.2502199</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Multimedia</title>
		<meeting>the 21st ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-10-21">2013</date>
			<biblScope unit="page" from="765" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Enhancement of Degraded Historical Kannada Documents</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gangamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikanta</forename><surname>Murthy K</surname></persName>
		</author>
		<idno type="DOI">10.5120/3692-5155</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<title level="j" type="abbrev">IJCA</title>
		<idno type="ISSNe">0975-8887</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2012">2012. 2012</date>
			<publisher>Foundation of Computer Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Linking unknown characters via oracle bone inscriptions retrieval</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runhua</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A Cross-Scenes Ancient Character Recognition Model Based on Normalized Generator and Transformer Encoder in Computational Archaeology</title>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fausto</forename><surname>Giunchiglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuntao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40494-023-00882-y</idno>
		<ptr target="https://ssrn.com/abstract=4741242" />
	</analytic>
	<monogr>
		<title level="j">Heritage Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ancient Ethiopic manuscripts character recognition using Deep Belief Networks</title>
		<author>
			<persName><forename type="first">Siranesh</forename><surname>Getu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneyachew</forename><surname>Tamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menore</forename><surname>Tekeba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zede Journal</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="37" to="52" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Script Recognition—A Review</title>
		<author>
			<persName><forename type="first">Debashis</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulika</forename><surname>Dube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adamane</forename><forename type="middle">P</forename><surname>Shivaprasad</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2010.30</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2142" to="2161" />
			<date type="published" when="2010-12">2010. 2010</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Towards a typology of phonemic scripts</title>
		<author>
			<persName><forename type="first">Amalia</forename><forename type="middle">E</forename><surname>Gnanadesikan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Writing Systems Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="35" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Optical character recognition for complex scripts: A case-study in cuneiform</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Gordin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Romach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In ADHO 2022-Tokyo</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Improving Handwritten Cyrillic OCR by Font-Based Synthetic Text Generator</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukáš</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Hlaváč</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Neduchal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marek</forename><surname>Hrúz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on the Dynamics of Information Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="102" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Zero-shot Generation of Training Data with Denoising Diffusion Probabilistic Model for Handwritten Chinese Character Recognition</title>
		<author>
			<persName><forename type="first">Dongnan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haisong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Huo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-41679-8_20</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="348" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Egyptian Hieroglyphs Segmentation with Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Guidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Python</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Forasassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costanza</forename><surname>Cucci</surname></persName>
			<idno type="ORCID">0000-0001-8534-7465</idno>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Franci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Argenti</surname></persName>
			<idno type="ORCID">0000-0001-7776-4015</idno>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Barucci</surname></persName>
			<idno type="ORCID">0000-0002-3759-7512</idno>
		</author>
		<idno type="DOI">10.3390/a16020079</idno>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<title level="j" type="abbrev">Algorithms</title>
		<idno type="ISSNe">1999-4893</idno>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2023-02-01">2023. 2023</date>
			<publisher>MDPI AG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">An Improved Neural Network Model Based on Inception-v3 for Oracle Bone Inscription Character Recognition</title>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Guo</surname></persName>
			<idno type="ORCID">0000-0002-0811-5086</idno>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingshuai</forename><surname>Liu</surname></persName>
			<idno type="ORCID">0000-0002-3630-7499</idno>
		</author>
		<author>
			<persName><forename type="first">Longquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingju</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Huang</surname></persName>
			<idno type="ORCID">0000-0002-2100-0259</idno>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
			<idno type="ORCID">0000-0002-8015-9091</idno>
		</author>
		<idno type="DOI">10.1155/2022/7490363</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Programming</title>
		<title level="j" type="abbrev">Scientific Programming</title>
		<idno type="ISSN">1058-9244</idno>
		<idno type="ISSNe">1875-919X</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2022-05-05">2022. 2022</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep Learning Classification of Large-Scale Point Clouds: A Case Study on Cuneiform Tablets</title>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Hagelskjaer</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip46576.2022.9898032</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-10-16">2022</date>
			<biblScope unit="page" from="826" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Self-supervised learning of Orc-Bert augmentator for recognizing few-shot oracle characters</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A Survey on Optical Character Recognition for Handwritten Devanagari Script Using Deep Learning</title>
		<author>
			<persName><forename type="first">Pragati</forename><surname>Hirugade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nidhi</forename><surname>Suryavanshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radhika</forename><surname>Bhagwat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smita</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rutwija</forename><surname>Phadke</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.4031738</idno>
	</analytic>
	<monogr>
		<title level="j">SSRN Electronic Journal</title>
		<title level="j" type="abbrev">SSRN Journal</title>
		<idno type="ISSNe">1556-5068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note>Smita Rajput, and Rutwija Phadke</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Automatic Maya hieroglyph retrieval using shape and context information</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">Pallan</forename><surname>Gayol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Krempel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<idno type="DOI">10.1145/2647868.2655044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-11-03">2014</date>
			<biblScope unit="page" from="1037" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Analyzing and visualizing ancient Maya hieroglyphics using shape: From computer vision to Digital Humanities</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">Pallán</forename><surname>Gayol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Scholarship in the Humanities</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="179" to="194" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">VGTS: Visually Guided Text Spotting for novel categories in historical manuscripts</title>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Hu</surname></persName>
			<idno type="ORCID">0000-0002-4197-8676</idno>
		</author>
		<author>
			<persName><forename type="first">Hongjian</forename><surname>Zhan</surname></persName>
			<idno type="ORCID">0000-0002-3906-658X</idno>
		</author>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Lu</surname></persName>
			<idno type="ORCID">0000-0003-4062-6553</idno>
		</author>
		<author>
			<persName><forename type="first">Ching</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
			<idno type="ORCID">0000-0003-1209-7631</idno>
		</author>
		<idno type="DOI">10.1016/j.eswa.2024.125557</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<title level="j" type="abbrev">Expert Systems with Applications</title>
		<idno type="ISSN">0957-4174</idno>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="page">125557</biblScope>
			<date type="published" when="2025-02">2025. 2025</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Agtgan: Unpaired image translation for photographic ancient character generation</title>
		<author>
			<persName><forename type="first">Hongxiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daihui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kin-Man</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchao</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM international conference on multimedia</title>
		<meeting>the 30th ACM international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5456" to="5467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">OBC306: A Large-Scale Oracle Bone Character Recognition Dataset</title>
		<author>
			<persName><forename type="first">Shuangping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdar.2019.00114</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-09">2019</date>
			<biblScope unit="volume">306</biblScope>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Comparison of different image denoising algorithms for Chinese calligraphy images</title>
		<author>
			<persName><forename type="first">Zhi-Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Biao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling-Ying</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page" from="102" to="112" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Chinese rubbing image binarization based on deep learning for image denoising</title>
		<author>
			<persName><forename type="first">Zhi-Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Mei</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling-Ying</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Control and Computer Vision</title>
		<meeting>the 2nd International Conference on Control and Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Processing phonographic and morphographic script: Similarities and differences. sl: sn</title>
		<author>
			<persName><forename type="first">Anja</forename><surname>Karina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ischebeck</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A hybrid deep learning model to recognize handwritten characters in ancient documents in Devanagari and Maithili scripts</title>
		<author>
			<persName><forename type="first">Amar</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajib</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="8389" to="8412" />
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Method for Qin Bamboo Slip Text Detection Based on an Enhanced DBNet Model</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huijuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 4th International Symposium on Artificial Intelligence and Intelligent Manufacturing (AIIM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="952" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">An Attempt at Zero-shot Ancient Documents Restoration Based on Diffusion Models</title>
		<author>
			<persName><forename type="first">Hayata</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuuya</forename><surname>Yoshizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryuto</forename><surname>Ishibashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Meng</surname></persName>
		</author>
		<idno type="DOI">10.1109/icamechs59878.2023.10272811</idno>
	</analytic>
	<monogr>
		<title level="m">2023 International Conference on Advanced Mechatronic Systems (ICAMechS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023-09-04">2023</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A comparative study of optical character recognition for Tamil script</title>
		<author>
			<persName><forename type="first">Kannan</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Scientific Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="570" to="582" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Self-adaptive hybridized lion optimization algorithm with transfer learning for ancient Tamil character recognition in stone inscriptions</title>
		<author>
			<persName><forename type="first">Indra</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiruba</forename><surname>Jaganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajith</forename><surname>Shankar Rameshbabu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubna</forename><forename type="middle">A</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gabralla</surname></persName>
		</author>
		<author>
			<persName><surname>Sivaraj</surname></persName>
		</author>
		<author>
			<persName><surname>Sm Nandhagopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="39621" to="39634" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">CNN-bidirectional LSTM based optical character recognition of Sanskrit manuscripts: A comprehensive systematic literature review</title>
		<author>
			<persName><forename type="first">Bhavesh</forename><surname>Kataria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Harikrishna</surname></persName>
		</author>
		<author>
			<persName><surname>Jethva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol.(IJSRCSEIT)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2456" to="3307" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Optical Character Recognition of Sanskrit Manuscripts Using Convolution Neural Networks</title>
		<author>
			<persName><forename type="first">Dr</forename><surname>Bhavesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kataria</forename><surname>Harikrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jethva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Webology</title>
		<idno type="ISSN">1735-188</idno>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Recognizing cuneiform signs using graph based methods</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nils M Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Fisseler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><surname>Weichert</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Cost-Sensitive Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Ancient Character Recognition: A Comprehensive Review</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krithiga</surname></persName>
			<idno type="ORCID">0000-0002-8842-1947</idno>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Varsini</surname></persName>
			<idno type="ORCID">0009-0001-0724-7634</idno>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Gabriel</forename><surname>Joshua</surname></persName>
			<idno type="ORCID">0009-0003-8649-9542</idno>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">U</forename><surname>Om Kumar</surname></persName>
			<idno type="ORCID">0000-0003-2866-0281</idno>
		</author>
		<idno type="DOI">10.1109/access.2023.3341352</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<title level="j" type="abbrev">IEEE Access</title>
		<idno type="ISSNe">2169-3536</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="88847" to="88857" />
			<date type="published" when="2023">2023. 2023</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Proposed Design to Recognize Ancient Sanskrit Manuscripts with Translation Using Machine Learning</title>
		<author>
			<persName><forename type="first">Ishwari</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swapnali</forename><surname>Tikkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Chaware</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Kharate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anushree</forename><surname>Pandit</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.4345688</idno>
	</analytic>
	<monogr>
		<title level="j">SSRN Electronic Journal</title>
		<title level="j" type="abbrev">SSRN Journal</title>
		<idno type="ISSNe">1556-5068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Decoupled Learning for Long-Tailed Oracle Character Recognition</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiu-Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-41685-9_11</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="165" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Towards better long-tailed oracle character recognition with adversarial data augmentation</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiu-Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Y</forename><surname>Goulermas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">109534</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">UCR: A unified character-radical dual-supervision framework for accurate Chinese character recognition</title>
		<author>
			<persName><forename type="first">Qilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongsheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">111373</biblScope>
			<date type="published" when="2025">2025. 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">SideNet: Learning representations from interactive side information for zero-shot Chinese character recognition</title>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dezhi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page">110208</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Deep Learning for Period Classification of Historical Hebrew Texts</title>
		<author>
			<persName><forename type="first">Chaya</forename><surname>Liebeskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmuel</forename><surname>Liebeskind</surname></persName>
		</author>
		<idno type="DOI">10.46298/jdmdh.5864</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Data Mining &amp; Digital Humanities</title>
		<idno type="ISSNe">2416-5999</idno>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
			<date type="published" when="2020-06-13">2020. 2020</date>
			<publisher>Centre pour la Communication Scientifique Directe (CCSD)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">An expert vision system for analysis of Hebrew characters and authentication of manuscripts</title>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Likforman-Sulem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henri</forename><surname>Maître</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colette</forename><surname>Sirat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="121" to="137" />
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Handwriting Trajectory Recovery Via Trajectory Transformer With Global Radical Context-Aware Module</title>
		<author>
			<persName><forename type="first">Junxiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhounan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangping</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-78498-9_13</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="182" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Recognition of OBIC&apos;s Variants by Using Deep Neural Networks and Spectral Clustering</title>
		<author>
			<persName><forename type="first">Guoying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingxin</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 4th International Conference on Information Systems and Computer Aided Education (ICISCAE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="39" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Spatial pyramid block for oracle bone inscription detection</title>
		<author>
			<persName><forename type="first">Guoying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jici</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 9th International Conference on Software and Computer Applications</title>
		<meeting>the 2020 9th International Conference on Software and Computer Applications</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="133" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Script Factors that Affect Literacy: Alphabetic vs. Logographic Languages</title>
		<author>
			<persName><forename type="first">In-Mao</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-94-011-1162-1_10</idno>
	</analytic>
	<monogr>
		<title level="m">Neuropsychology and Cognition</title>
		<imprint>
			<publisher>Springer Netherlands</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="145" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Oracle Bone Inscriptions Recognition Based on Deep Convolutional Neural Network</title>
		<author>
			<persName><forename type="first">Mengting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingju</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.18178/joig.8.4.114-119</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Image and Graphics</title>
		<title level="j" type="abbrev">JOIG</title>
		<idno type="ISSN">2301-3699</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="114" to="119" />
			<date type="published" when="2020">2020. 2020</date>
			<publisher>EJournal Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">One shot ancient character recognition with siamese similarity network</title>
		<author>
			<persName><forename type="first">Xuxing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rankang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanxiong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14820</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Handwritten Vedic Sanskrit text recognition using deep learning</title>
		<author>
			<persName><forename type="first">Ms</forename><surname>Vina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lomte</surname></persName>
		</author>
		<author>
			<persName><surname>Dharmpal D Doye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algebraic Statistics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2190" to="2198" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Self-information of radicals: A new clue for zero-shot Chinese character recognition</title>
		<author>
			<persName><forename type="first">Guo-Feng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Han</forename><surname>Wang</surname></persName>
			<idno type="ORCID">0000-0002-5901-0778</idno>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua-Yi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu-Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunzhi</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2023.109598</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<title level="j" type="abbrev">Pattern Recognition</title>
		<idno type="ISSN">0031-3203</idno>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">109598</biblScope>
			<date type="published" when="2023-08">2023. 2023</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Joint radical embedding and detection for zero-shot Chinese character recognition</title>
		<author>
			<persName><forename type="first">Guo-Feng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu-Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Hao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunzhi</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page">111286</biblScope>
			<date type="published" when="2025">2025. 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Deciphering Undersegmented Ancient Scripts Using Phonetic Prior</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00354</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<title level="j" type="abbrev">Transactions of the Association for Computational Linguistics</title>
		<idno type="ISSNe">2307-387X</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="69" to="81" />
			<date type="published" when="2021-02">2021. 2021</date>
			<publisher>MIT Press - Journals</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">End-to-end historical handwritten ethiopic text recognition using deep learning</title>
		<author>
			<persName><forename type="first">Ruchika</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maru</forename><surname>Tesfaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Addis</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="99535" to="99545" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Tamil character recognition from ancient epigraphical inscription using OCR and NLP</title>
		<author>
			<persName><surname>Tvvdvnb Manigandan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vidhya</surname></persName>
		</author>
		<author>
			<persName><surname>Dhanalakshmi</surname></persName>
		</author>
		<author>
			<persName><surname>Nirmala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on energy, communication, data analytics and soft computing (ICECDS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1008" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Breaking the code on broken tablets: The learning challenge for annotated cuneiform script in normalized 2d and 3d datasets</title>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Mara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Bogacz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="148" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A Convolutional Recurrent Neural Network for the Handwritten Text Recognition of Historical Greek Manuscripts</title>
		<author>
			<persName><forename type="first">K</forename><surname>Markou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tsochatzidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zagoris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Symeonidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-68787-8_18</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021">January 10-15, 2021</date>
			<biblScope unit="page" from="249" to="262" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Recognition of Oracle Bone Inscriptions by Extracting Line Features on Image Processing</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPRAM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="606" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Recognition of oracle bone inscriptions using deep learning based on data augmentation. In 2018 metrology for archaeology and cultural heritage (MetroArchaeo)</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Kamitoku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsuhiro</forename><surname>Yamazaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">A transformer-based approach for Arabic offline handwritten text recognition. Signal</title>
		<author>
			<persName><forename type="first">Saleh</forename><surname>Momeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bagher</forename><surname>Babaali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="3053" to="3062" />
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">OCFormer: A Transformer-Based Model For Arabic Handwritten Text Recognition</title>
		<author>
			<persName><forename type="first">Aly</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elbehery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salma</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghada</forename><surname>Khoriba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><forename type="middle">S</forename><surname>Ghoneim</surname></persName>
		</author>
		<idno type="DOI">10.1109/miucc52538.2021.9447608</idno>
	</analytic>
	<monogr>
		<title level="m">2021 International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-05-26">2021</date>
			<biblScope unit="page" from="182" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Intelligent recognition of ancient Persian cuneiform characters</title>
		<author>
			<persName><forename type="first">Fahimeh</forename><surname>Mostofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Khashman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Computation Theory and Applications</title>
		<imprint>
			<publisher>SCITEPRESS</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="119" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">CNN -LSTM Based Approach for Recognition of Devanagari Manuscripts</title>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Moudgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saravjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavna</forename><surname>Sareen</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdabi56818.2022.10041262</idno>
	</analytic>
	<monogr>
		<title level="m">2022 International Conference on Data Analytics for Business and Industry (ICDABI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-10-25">2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Devanagari Ancient Manuscript Recognition Using AlexNet</title>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Moudgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saravjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavna</forename><surname>Sareen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Wadhwa</surname></persName>
		</author>
		<idno type="DOI">10.1109/icrito61523.2024.10522438</idno>
	</analytic>
	<monogr>
		<title level="m">2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024-03-14">2024</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Extracting old persian cuneiform font out of noisy images (handwritten or inscription)</title>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vyacheslav</forename><surname>Lyashenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 10th Iranian Conference on Machine Vision and Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="241" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Application of phase-based features and denoising in postprocessing and binarization of historical document images</title>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Ziaei Nafchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><forename type="middle">Farrahi</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Cheriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A novel stage wise denoising approach on ancient Kannada script from rock images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Bj Bipin Nair</surname></persName>
		</author>
		<author>
			<persName><surname>Anusha</surname></persName>
		</author>
		<author>
			<persName><surname>Anusha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 7th International Conference on Communication and Electronics Systems (ICCES)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1715" to="1723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">A two phase denoising approach to remove uneven illumination from ancient note book images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bj Bipin Nair</surname></persName>
		</author>
		<author>
			<persName><surname>Shobharani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gopikrishna</forename><surname>Sreekumar</surname></persName>
		</author>
		<author>
			<persName><surname>Ashok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 7th International Conference on Advanced Computing and Communication Systems (ICACCS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1563" to="1568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Devanagari ancient character recognition using DCT features with adaptive boosting and bootstrap aggregating</title>
		<author>
			<persName><forename type="first">Rani</forename><surname>Sonika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><forename type="middle">Kumar</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munish</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="13603" to="13614" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Ancient text recognition: a review</title>
		<author>
			<persName><forename type="first">Rani</forename><surname>Sonika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><forename type="middle">Kumar</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munish</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="5517" to="5558" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Bangla sign alphabet recognition with zero-shot and transfer learning</title>
		<author>
			<persName><forename type="first">Ragib</forename><surname>Amin Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sejuti</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nawara</forename><forename type="middle">Mahmood</forename><surname>Broti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamim</forename><surname>Ahmed Deowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="84" to="93" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Handwritten Kazakh and Russian (HKR) database for text recognition</title>
		<author>
			<persName><forename type="first">Daniyar</forename><surname>Nurseitov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kairat</forename><surname>Bostanbekov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniyar</forename><surname>Kurmankhojayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anel</forename><surname>Alimova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Abdallah</surname></persName>
			<idno type="ORCID">0000-0001-8747-4927</idno>
		</author>
		<author>
			<persName><forename type="first">Rassul</forename><surname>Tolegenov</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-021-11399-6</idno>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<title level="j" type="abbrev">Multimed Tools Appl</title>
		<idno type="ISSN">1380-7501</idno>
		<idno type="ISSNe">1573-7721</idno>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">21-23</biblScope>
			<biblScope unit="page" from="33075" to="33097" />
			<date type="published" when="2021-08-13">2021</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Qin Seal Script Character Recognition with Fuzzy and Incomplete Information</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di-Wen</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue-Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Baghdad Science Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">SI</biblScope>
			<biblScope unit="page" from="696" to="0696" />
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Automatic writer identification of ancient Greek inscriptions</title>
		<author>
			<persName><forename type="first">Michail</forename><surname>Panagopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Papaodysseus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panayiotis</forename><surname>Rousopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitra</forename><surname>Dafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tracy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1404" to="1414" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Handwriting automatic classification: Application to ancient Greek inscriptions</title>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Papaodysseus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panayiotis</forename><surname>Rousopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Arabadjis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fivi</forename><surname>Panopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalis</forename><surname>Panagopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 International Conference on Autonomous and Intelligent Systems, AIS 2010</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Handwritten paleographic greek text recognition: a century-based approach</title>
		<author>
			<persName><forename type="first">Paraskevi</forename><surname>Platanou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Papaioannou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Language Resources and Evaluation Conference</title>
		<meeting>the Thirteenth Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6585" to="6589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">ARsinoë - Learning Egyptian Hieroglyphs with Augmented Reality and Machine Learning</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Plecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><forename type="middle">M</forename><surname>Seyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gudrun</forename><surname>Klinker</surname></persName>
		</author>
		<idno type="DOI">10.1109/ismar-adjunct51615.2020.00092</idno>
		<ptr target="https://doi.org/10.1109/ISMAR-Adjunct51615.2020.00092" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-11">2020</date>
			<biblScope unit="page" from="326" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Digitizing Cyrillic Manuscripts for the Historical Dictionary of the Serbian Language Using Handwritten Text Recognition Technology</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Polomac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Kurešević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isidora</forename><surname>Bjelaković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Slavic Studies</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="295" to="316" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Aleksandra Colić Jovanović, and Sanja Petrović</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Handwritten Tamil Character Recognition UsingDeep Learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Pragathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Priyadarshini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shavar</forename><surname>Saveetha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Shavar</forename><surname>Banu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Mohammed Aarif</surname></persName>
		</author>
		<idno type="DOI">10.1109/vitecon.2019.8899614</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Vision Towards Emerging Trends in Communication and Networking (ViTECoN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-03">2019</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">An Efficient, Font Independent Word and Character Segmentation Algorithm for Printed Arabic Text</title>
		<author>
			<persName><forename type="first">Aziz</forename><surname>Qaroush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bassam</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khader</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Washha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eman</forename><surname>Maali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nibal</forename><surname>Nayef</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jksuci.2019.08.013</idno>
		<ptr target="https://doi.org/10.1016/j.jksuci.2019.08.013" />
	</analytic>
	<monogr>
		<title level="j">Journal of King Saud University -Computer and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">08</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Case study in Hebrew character searching</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Rabaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofer</forename><surname>Biller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihad</forename><surname>El-Sana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klara</forename><surname>Kedem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itshak</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1080" to="1084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Century identification and recognition of ancient Tamil character recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rajakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharathi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="32" to="35" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Mixed Model OCR Training on Historical Latin Script for Out-of-the-Box Recognition and Finetuning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Reul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Noeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Buettner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Wehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uwe</forename><surname>Springmann</surname></persName>
		</author>
		<idno type="DOI">10.1145/3476887.3476910</idno>
	</analytic>
	<monogr>
		<title level="m">The 6th International Workshop on Historical Document Imaging and Processing</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-09-05">2021</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Transferring Neural Representations for Low-Dimensional Indexing of Maya Hieroglyphic Art</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Roman-Rangel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gulcan</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephane</forename><surname>Marchand-Maillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">Pallán</forename><surname>Gayol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Krempel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Spotak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46604-0_58</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016-10-08">2016. October 8-10 and 15-16, 2016</date>
			<biblScope unit="page" from="842" to="855" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 14</note>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Retrieving ancient maya glyphs with shape context</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Roman-Rangel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Pallan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="988" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Analyzing Ancient Maya Glyph Collections with Contextual Shape Descriptors</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Roman-Rangel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Pallan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-010-0387-x</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<title level="j" type="abbrev">Int J Comput Vis</title>
		<idno type="ISSN">0920-5691</idno>
		<idno type="ISSNe">1573-1405</idno>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="117" />
			<date type="published" when="2011">2011. 2011</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Searching the past: an improved shape descriptor to retrieve maya hieroglyphs</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Roman-Rangel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">Pallan</forename><surname>Gayol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM international conference on Multimedia</title>
		<meeting>the 19th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Optical Character Recognition of 19th Century Classical Commentaries: the Current State of Affairs</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Romanello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Najem-Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3476887.3476911</idno>
	</analytic>
	<monogr>
		<title level="m">The 6th International Workshop on Historical Document Imaging and Processing</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-09-05">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">A new approach for ancient inscriptions&apos; writer identification</title>
		<author>
			<persName><forename type="first">Panayiotis</forename><surname>Rousopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michail</forename><surname>Panagopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Papaodysseus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fivi</forename><surname>Panopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Arabadjis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tracy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fotios</forename><surname>Giannopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solomon</forename><surname>Zannos</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdsp.2011.6004966</idno>
	</analytic>
	<monogr>
		<title level="m">2011 17th International Conference on Digital Signal Processing (DSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-07">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Seal detection and recognition: An approach for document indexing</title>
		<author>
			<persName><forename type="first">Partha</forename><surname>Pratim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Umapada</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Lladós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 10th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="101" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Muharaf: Manuscripts of handwritten arabic dataset for cursive text recognition</title>
		<author>
			<persName><forename type="first">Mehreen</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anupam</forename><surname>Mijar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerges</forename><surname>Habchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Younes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau-Wai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akram</forename><surname>Khater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="58525" to="58538" />
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Devnagari handwritten character recognition (DHCR) for ancient documents: a review</title>
		<author>
			<persName><forename type="first">Ravindra</forename><surname>Kunal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipak</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Badgujar</forename><surname>Dattatray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Information &amp; Communication Technologies</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="656" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">A novel SVM-based handwritten Tamil character recognition system</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shanthi</surname></persName>
		</author>
		<author>
			<persName><surname>Duraiswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="173" to="180" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Automatic Clustering of Hebrew Manuscripts</title>
		<author>
			<persName><forename type="first">Daria</forename><surname>Vasyutinsky Shapira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berat</forename><surname>Kurar-Barakat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Suliman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharva</forename><surname>Gogawale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nachum</forename><surname>Dershowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Digital Research Infrastructure for the Arts and Humanities</title>
		<meeting>eeding of the Digital Research Infrastructure for the Arts and Humanities</meeting>
		<imprint>
			<publisher>DARIAH) Annual Events</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">The Challenges of Recognizing Offline Handwritten Chinese: A Technical Review</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Shen</surname></persName>
			<idno type="ORCID">0000-0002-2002-7118</idno>
		</author>
		<author>
			<persName><forename type="first">Bidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-Kit</forename><surname>Tang</surname></persName>
			<idno type="ORCID">0000-0001-8104-7887</idno>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Mirri</surname></persName>
			<idno type="ORCID">0000-0002-5385-4734</idno>
		</author>
		<idno type="DOI">10.3390/app13063500</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<title level="j" type="abbrev">Applied Sciences</title>
		<idno type="ISSNe">2076-3417</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3500</biblScope>
			<date type="published" when="2023-03-09">2023. 2023</date>
			<publisher>MDPI AG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">CharFormer: A Glyph Fusion based Attentive Framework for High-precision Character Image Denoising</title>
		<author>
			<persName><forename type="first">Daqian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lida</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuntao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3503161.3548208</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-10-10">2022</date>
			<biblScope unit="page" from="1147" to="1155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">RCRN: Real-world character image restoration network via skeleton extraction</title>
		<author>
			<persName><forename type="first">Daqian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaomin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM international conference on multimedia</title>
		<meeting>the 30th ACM international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1177" to="1185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">An integrated method for ancient Chinese tablet images de-noising based on assemble of multiple image smoothing filters</title>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghua</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="12245" to="12261" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">A Chinese character structure preserved denoising method for Chinese tablet calligraphy document images based on KSVD dictionary learning</title>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghua</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="14921" to="14936" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="http://citeseerx.ist.psu.edu/viewdoc/summary" />
		<title level="m">Machine Learning Final Project: Handwritten Sanskrit Recognition using a Multi-class SVM with K-NN Guidance</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Recognition of historical Greek polytonic scripts using LSTM networks</title>
		<author>
			<persName><forename type="first">Fotini</forename><surname>Simistira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Ul-Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassilis</forename><surname>Papavassiliou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basilis</forename><surname>Gatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassilis</forename><surname>Katsouros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 13th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="766" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title level="m" type="main">Youtham Boules, and Nermin Negied. 2023. An AI Based Automatic Translator for Ancient Hieroglyphic Language -From Scanned Images to English Text</title>
		<author>
			<persName><forename type="first">Asmaa</forename><surname>Sobhy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Helmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Elmasry</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2023.3267981</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2023.3267981" />
		<imprint>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
	<note>IEEE Access PP (01 2023</note>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">OCR of historical printings of Latin texts: problems, prospects, progress</title>
		<author>
			<persName><forename type="first">Uwe</forename><surname>Springmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Najock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Morgenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annette</forename><surname>Gotscharek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First international conference on digital access to textual cultural heritage</title>
		<meeting>the First international conference on digital access to textual cultural heritage</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="71" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Stötzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Homburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Mara</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccvw60793.2023.00183</idno>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023-10-02">2023</date>
			<biblScope unit="page" from="1672" to="1680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Tamilbrahmi script character recognition system using deep learning technique</title>
		<author>
			<persName><surname>Subadivya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vigneswari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yaminie</surname></persName>
		</author>
		<author>
			<persName><surname>Diviya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Science and Mobile Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="114" to="119" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Toward a period-specific optimized neural network for OCR error correction of historical Hebrew texts</title>
		<author>
			<persName><forename type="first">Omri</forename><surname>Suissa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maayan</forename><surname>Zhitomirsky-Geffet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avshalom</forename><surname>Elmalech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Journal on Computing and Cultural Heritage (JOCCH)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Dual-view oracle bone script recognition system via temporal-spatial psychovisual modulation</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongpai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingzhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaodi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="193" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">Advancing Digital Papyrology: Machine Learning and Blockchain Tools for Modernizing the Study of Ancient Greek Manuscripts</title>
		<author>
			<persName><forename type="first">I</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><surname>Swindall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
		<respStmt>
			<orgName>Middle Tennessee State University</orgName>
		</respStmt>
	</monogr>
	<note>Ph. D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Exploring learning approaches for ancient Greek character recognition with citizen science data</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Matthew I Swindall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chase</forename><forename type="middle">C</forename><surname>Croisdale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Keener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nita</forename><surname>Brusuelas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Krevans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Sellew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">F</forename><surname>Fortson</surname></persName>
		</author>
		<author>
			<persName><surname>Wallin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="128" to="137" />
			<date type="published" when="2021">2021. 2021</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Dataset Augmentation in Papyrology with Generative Models: A Study of Synthetic Ancient Greek Character Images</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Matthew I Swindall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Player</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Keener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federica</forename><surname>Brusuelas</surname></persName>
		</author>
		<author>
			<persName><surname>Nicolardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Marzia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Angelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vergara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">F</forename><surname>Mcosker</surname></persName>
		</author>
		<author>
			<persName><surname>Wallin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4973" to="4979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Gated Convolution and Stacked Self-Attention Encoder-Decoder-Based Model for Offline Handwritten Ethiopic Text Recognition</title>
		<author>
			<persName><forename type="first">Direselign</forename><surname>Addis Tadesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan-Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van-Dai</forename><surname>Ta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">654</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Ancient Character Recognition: A Novel Image Dataset of Shui Manuscript Characters and Classification Model</title>
		<author>
			<persName><forename type="first">Minli</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaomin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.23919/cje.2022.00.077</idno>
	</analytic>
	<monogr>
		<title level="j">Chinese Journal of Electronics</title>
		<title level="j" type="abbrev">Chinese J. Elect.</title>
		<idno type="ISSN">1022-4653</idno>
		<idno type="ISSNe">2075-5597</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="75" />
			<date type="published" when="2023-01">2023. 2023</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Isolated Handwritten Character Recognition of Ancient Hebrew Manuscripts</title>
		<author>
			<persName><surname>Tabita L Tobing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sony</forename><surname>Sule Y Yayilgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torleif</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><surname>Elgvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Archiving Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="35" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Common and script-specific awareness in relation to word recognition in English and Chinese</title>
		<author>
			<persName><forename type="first">Etsuko</forename><surname>Toyoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Scrimgeour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Awareness</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="61" to="73" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Alexandros Papazoglou, and Ioannis Pratikakis. 2021. Htr for greek historical handwritten documents</title>
		<author>
			<persName><forename type="first">Lazaros</forename><surname>Tsochatzidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Symeon</forename><surname>Symeonidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of imaging</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">260</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Bronze Culture Image Recognition System based on Artificial Intelligence and Network Technology</title>
		<author>
			<persName><forename type="first">Mixiao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1088/1742-6596/1574/1/012097</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics: Conference Series</title>
		<title level="j" type="abbrev">J. Phys.: Conf. Ser.</title>
		<idno type="ISSN">1742-6588</idno>
		<idno type="ISSNe">1742-6596</idno>
		<imprint>
			<biblScope unit="volume">1574</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">012097</biblScope>
			<date type="published" when="2020-06-01">2020</date>
			<publisher>IOP Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Study on the evolution of Chinese characters based on few-shot learning: From oracle bone inscriptions to regular script</title>
		<author>
			<persName><forename type="first">Mengru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruichen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingju</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plos one</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">272974</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">A dataset of oracle characters for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihong</forename><surname>Deng</surname></persName>
			<idno type="ORCID">0000-0001-5952-6996</idno>
		</author>
		<idno type="DOI">10.1038/s41597-024-02933-w</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<title level="j" type="abbrev">Sci Data</title>
		<idno type="ISSNe">2052-4463</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">87</biblScope>
			<date type="published" when="2024-01-18">2024. 2024</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Unsupervised Structure-Texture Separation Network for Oracle Character Recognition</title>
		<author>
			<persName><forename type="first">Mei</forename><surname>Wang</surname></persName>
			<idno type="ORCID">0000-0002-3559-9346</idno>
		</author>
		<author>
			<persName><forename type="first">Weihong</forename><surname>Deng</surname></persName>
			<idno type="ORCID">0000-0001-5952-6996</idno>
		</author>
		<author>
			<persName><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
			<idno type="ORCID">0000-0002-6743-4175</idno>
		</author>
		<idno type="DOI">10.1109/tip.2022.3165989</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<title level="j" type="abbrev">IEEE Trans. on Image Process.</title>
		<idno type="ISSN">1057-7149</idno>
		<idno type="ISSNe">1941-0042</idno>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3137" to="3150" />
			<date type="published" when="2022">2022. 2022</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Oracle character recognition using unsupervised discriminative consistency network</title>
		<author>
			<persName><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page">110180</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Oracle bone inscriptions detection in rubbings based on deep learning</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingju</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinxian</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1671" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">A gan-based denoising method for chinese stele and rubbing calligraphic image</title>
		<author>
			<persName><forename type="first">Xuanhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The visual computer</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1351" to="1362" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">EA-GAN: restoration of text in ancient Chinese books based on an example attention generative adversarial network</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wenjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><surname>Benpeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Ruiqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xihua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Shanxiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heritage Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">DeepScribe: Localization and Classification of Elamite Cuneiform Signs Via Deep Learning</title>
		<author>
			<persName><forename type="first">Williams</forename><surname>Edward C</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><forename type="middle">R</forename><surname>Schloen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Miller C Prosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM J. Comput. Cult. Herit</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Ancient Chinese recognition method based on attention mechanism</title>
		<author>
			<persName><forename type="first">Lingjing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengqiu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 7th IEEE International Conference on Network Intelligence and Digital Content</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="309" to="313" />
		</imprint>
		<respStmt>
			<orgName>IC-NIDC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">CNN-based Bronze Inscriptions Character Recognition</title>
		<author>
			<persName><forename type="first">Xuanqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/aemcse55572.2022.00106</idno>
	</analytic>
	<monogr>
		<title level="m">2022 5th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-04">2022</date>
			<biblScope unit="page" from="514" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">DiffOBI: Diffusion-based Image Generation of Oracle Bone Inscription Style Characters</title>
		<author>
			<persName><forename type="first">Xiaoxuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xusheng</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2024 Technical Communications</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Oracle bone inscription detection: a survey of oracle bone inscription detection based on deep learning algorithm</title>
		<author>
			<persName><forename type="first">Jici</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing</title>
		<meeting>the International Conference on Artificial Intelligence, Information Processing and Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Chinese-Seal Dataset(Csd): Diverse Attributes for Seal Detection, Segmentation and Recognition</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangshuang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.5079459</idno>
	</analytic>
	<monogr>
		<title level="m">Chinese-Seal Dataset (Csd): Diverse Attributes for Seal Detection, Segmentation and Recognition. Segmentation and Recognition</title>
		<imprint>
			<publisher>Elsevier BV</publisher>
			<date type="published" when="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Research on denoising method of chinese ancient character image based on chinese character writing standard model</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Miao Yalin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yichun</surname></persName>
		</author>
		<author>
			<persName><surname>Guodong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2022">2022. 2022. 19795</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">ICDAR 2013 Chinese handwriting recognition competition</title>
		<author>
			<persName><forename type="first">Qiu-Feng</forename><surname>Fei Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 12th international conference on document analysis and recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1464" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Phonology facilitates deeply opaque logographic writing</title>
		<author>
			<persName><forename type="first">Mio</forename><surname>Yokoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kouji</forename><surname>Takano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimihiro</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plos one</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">312471</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Chinese character recognition with radical-structured stroke trees</title>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingye</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
			<idno type="ORCID">0000-0002-9633-0033</idno>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-023-06450-6</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<title level="j" type="abbrev">Mach Learn</title>
		<idno type="ISSN">0885-6125</idno>
		<idno type="ISSNe">1573-0565</idno>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3807" to="3827" />
			<date type="published" when="2024">2024. 2024</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Ancient character detection based on fine-grained density map</title>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzhen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuntao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Heritage Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2025">2025. 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Deep learning and image processing combined organization of shirakawa&apos;s hand-notated documents on OBI research</title>
		<author>
			<persName><forename type="first">Xuebin</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshiyuki</forename><surname>Fujikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Networking, Sensing and Control (ICNSC)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Zero-Shot Chinese Character Recognition with Stroke- and Radical-Level Decompositions</title>
		<author>
			<persName><forename type="first">Jinshan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiying</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ijcnn54540.2023.10191050</idno>
	</analytic>
	<monogr>
		<title level="m">2023 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023-06-18">2023</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">FaRE: A Feature-Aware Radical Encoding Strategy for Zero-Shot Chinese Character Recognition</title>
		<author>
			<persName><forename type="first">Hongjian</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Jie</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="390" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Data-driven oracle bone rejoining: A dataset and practical self-supervised learning scheme</title>
		<author>
			<persName><forename type="first">Chongsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-Feng</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Almpanidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4482" to="4492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">AI-powered oracle bone inscriptions recognition and fragments rejoining</title>
		<author>
			<persName><forename type="first">Chongsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bofeng</forename><surname>Mo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5309" to="5311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">A novel generative adversarial net for calligraphic tablet images denoising</title>
		<author>
			<persName><forename type="first">Jiulong</forename><surname>Zhang</surname></persName>
			<idno type="ORCID">0000-0001-6886-1809</idno>
		</author>
		<author>
			<persName><forename type="first">Mingtao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-019-08052-8</idno>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<title level="j" type="abbrev">Multimed Tools Appl</title>
		<idno type="ISSN">1380-7501</idno>
		<idno type="ISSNe">1573-7721</idno>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="119" to="140" />
			<date type="published" when="2020">2020. 2020</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Radical Analysis Network for Zero-Shot Learning in Printed Chinese Character Recognition</title>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Dai</surname></persName>
		</author>
		<idno type="DOI">10.1109/icme.2018.8486456</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-07">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Yongshun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v35i4.16458</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3447" to="3455" />
			<date type="published" when="2021-05-18">2021</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Oracle Character Recognition by Nearest Neighbor Classification with Deep Metric Learning</title>
		<author>
			<persName><forename type="first">Yi-Kang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Ge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdar.2019.00057</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-09">2019</date>
			<biblScope unit="page" from="309" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Improvement of ancient Shui character recognition model based on convolutional neural network</title>
		<author>
			<persName><forename type="first">Hongshuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhen</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="33080" to="33087" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">An Oracle Bone Inscriptions Detection Algorithm Based on Improved YOLOv8</title>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">174</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Style-independent radical sequence learning for zero-shot recognition of Small Seal script</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumiyo</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojun</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Franklin Institute</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="page" from="11295" to="11313" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
